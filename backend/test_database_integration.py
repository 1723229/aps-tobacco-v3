"""
æµ‹è¯•æ•°æ®åº“é›†æˆ - éªŒè¯æ‰€æœ‰ç®—æ³•éƒ½èƒ½æ­£ç¡®ä½¿ç”¨çœŸå®æ•°æ®åº“æ•°æ®
"""
import asyncio
import sys
import os
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.algorithms.pipeline import AlgorithmPipeline
from app.services.database_query_service import DatabaseQueryService


async def test_database_connectivity():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    print("ğŸ”— æµ‹è¯•æ•°æ®åº“è¿æ¥...")
    
    try:
        # æµ‹è¯•åŸºç¡€æŸ¥è¯¢
        decade_plans = await DatabaseQueryService.get_decade_plans()
        machine_relations = await DatabaseQueryService.get_machine_relations()
        machine_speeds = await DatabaseQueryService.get_machine_speeds()
        shift_configs = await DatabaseQueryService.get_shift_config()
        
        print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        print(f"   æ—¬è®¡åˆ’æ•°æ®: {len(decade_plans)}æ¡")
        print(f"   æœºå°å…³ç³»: {len(machine_relations)}ä¸ªå–‚ä¸æœº")
        print(f"   æœºå°é€Ÿåº¦: {len(machine_speeds)}å°æœºå™¨")
        print(f"   ç­æ¬¡é…ç½®: {len(shift_configs)}ä¸ªç­æ¬¡")
        
        return True, {
            'decade_plans_count': len(decade_plans),
            'machine_relations_count': len(machine_relations),
            'machine_speeds_count': len(machine_speeds),
            'shift_configs_count': len(shift_configs)
        }
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        return False, str(e)


async def test_pipeline_with_real_data():
    """æµ‹è¯•ä½¿ç”¨çœŸå®æ•°æ®åº“æ•°æ®çš„å®Œæ•´ç®¡é“"""
    print("\nğŸ­ æµ‹è¯•ä½¿ç”¨çœŸå®æ•°æ®åº“æ•°æ®çš„ç®—æ³•ç®¡é“...")
    
    try:
        pipeline = AlgorithmPipeline()
        
        # æŸ¥æ‰¾æœ€è¿‘çš„æ‰¹æ¬¡æ•°æ®
        decade_plans = await DatabaseQueryService.get_decade_plans()
        
        if not decade_plans:
            print("âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰æ—¬è®¡åˆ’æ•°æ®ï¼Œæ— æ³•æµ‹è¯•çœŸå®æ•°æ®æµ")
            return False, "No decade plan data found"
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ID
        first_batch_id = decade_plans[0]['import_batch_id']
        print(f"   ä½¿ç”¨æ‰¹æ¬¡ID: {first_batch_id}")
        
        # æ‰§è¡Œä½¿ç”¨çœŸå®æ•°æ®çš„å®Œæ•´ç®¡é“
        result = await pipeline.execute_full_pipeline_with_batch(
            import_batch_id=first_batch_id,
            use_real_data=True
        )
        
        if result['success']:
            final_work_orders = result['final_work_orders']
            feeder_orders = [wo for wo in final_work_orders if wo.get('work_order_type') == 'HWS']
            maker_orders = [wo for wo in final_work_orders if wo.get('work_order_type') == 'HJB']
            
            print(f"âœ… çœŸå®æ•°æ®ç®¡é“æµ‹è¯•æˆåŠŸ!")
            print(f"   è¾“å…¥æ‰¹æ¬¡: {first_batch_id}")
            print(f"   æ‰§è¡Œæ—¶é—´: {result['execution_duration_seconds']:.2f}ç§’")
            print(f"   è¾“å…¥è®°å½•: {result['summary']['input_records']}æ¡")
            print(f"   è¾“å‡ºå·¥å•: {result['summary']['output_work_orders']}æ¡")
            print(f"   å–‚ä¸æœºå·¥å•: {len(feeder_orders)}ä¸ª")
            print(f"   å·åŒ…æœºå·¥å•: {len(maker_orders)}ä¸ª")
            
            # éªŒè¯å…³é”®éœ€æ±‚
            assert len(feeder_orders) > 0, "å¿…é¡»ç”Ÿæˆå–‚ä¸æœºå·¥å•"
            assert len(maker_orders) > 0, "å¿…é¡»ç”Ÿæˆå·åŒ…æœºå·¥å•"
            
            return True, result
        else:
            print(f"âŒ çœŸå®æ•°æ®ç®¡é“æµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}")
            return False, result.get('error', 'Unknown error')
            
    except Exception as e:
        print(f"âŒ çœŸå®æ•°æ®ç®¡é“æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False, str(e)


async def test_individual_algorithms_with_real_data():
    """æµ‹è¯•å„ä¸ªç®—æ³•æ¨¡å—çš„æ•°æ®åº“é›†æˆ"""
    print("\nğŸ§© æµ‹è¯•å„ç®—æ³•æ¨¡å—çš„æ•°æ®åº“é›†æˆ...")
    
    from app.algorithms.data_preprocessing import DataPreprocessor
    from app.algorithms.merge_algorithm import MergeAlgorithm
    from app.algorithms.split_algorithm import SplitAlgorithm
    from app.algorithms.time_correction import TimeCorrection
    from app.algorithms.parallel_processing import ParallelProcessing
    from app.algorithms.work_order_generation import WorkOrderGeneration
    
    try:
        # è·å–ä¸€äº›æµ‹è¯•æ•°æ®
        decade_plans = await DatabaseQueryService.get_decade_plans()
        if not decade_plans:
            print("âš ï¸ æ²¡æœ‰æ•°æ®è¿›è¡Œæµ‹è¯•")
            return False, "No test data"
        
        test_data = decade_plans[:3]  # ä½¿ç”¨å‰3æ¡æ•°æ®æµ‹è¯•
        current_data = test_data
        
        algorithms = [
            ("æ•°æ®é¢„å¤„ç†", DataPreprocessor()),
            ("è§„åˆ™åˆå¹¶", MergeAlgorithm()),
            ("è§„åˆ™æ‹†åˆ†", SplitAlgorithm()),
            ("æ—¶é—´æ ¡æ­£", TimeCorrection()),
            ("å¹¶è¡Œåˆ‡åˆ†", ParallelProcessing()),
            ("å·¥å•ç”Ÿæˆ", WorkOrderGeneration())
        ]
        
        test_results = []
        
        for name, algorithm in algorithms:
            print(f"   æµ‹è¯• {name}...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰process_with_real_dataæ–¹æ³•
            if hasattr(algorithm, 'process_with_real_data'):
                result = await algorithm.process_with_real_data(current_data)
            else:
                result = algorithm.process(current_data)
            
            if result.success:
                print(f"   âœ… {name}: è¾“å…¥{len(current_data)} -> è¾“å‡º{len(result.output_data)}")
                current_data = result.output_data
                test_results.append({
                    'name': name,
                    'success': True,
                    'input_count': len(current_data),
                    'output_count': len(result.output_data),
                    'used_real_data': result.metrics.custom_metrics.get('used_real_database_data', False)
                })
            else:
                print(f"   âŒ {name}: {result.error_summary}")
                test_results.append({
                    'name': name,
                    'success': False,
                    'error': result.error_summary
                })
                break
        
        # ç»Ÿè®¡ä½¿ç”¨çœŸå®æ•°æ®çš„ç®—æ³•æ•°é‡
        real_data_algorithms = sum(1 for r in test_results if r.get('used_real_data', False))
        total_algorithms = len(test_results)
        
        print(f"\n   ğŸ“Š ç®—æ³•æµ‹è¯•æ‘˜è¦:")
        print(f"   æ€»ç®—æ³•æ•°: {total_algorithms}")
        print(f"   ä½¿ç”¨çœŸå®æ•°æ®: {real_data_algorithms}")
        print(f"   æ•°æ®åº“é›†æˆåº¦: {real_data_algorithms/total_algorithms*100:.1f}%")
        
        return True, test_results
        
    except Exception as e:
        print(f"âŒ ç®—æ³•æ¨¡å—æµ‹è¯•å¤±è´¥: {str(e)}")
        return False, str(e)


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª APSç®—æ³•æ•°æ®åº“é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    all_success = True
    
    # 1. æµ‹è¯•æ•°æ®åº“è¿æ¥
    db_success, db_result = await test_database_connectivity()
    if not db_success:
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return 1
    
    # 2. æµ‹è¯•å„ç®—æ³•æ¨¡å—çš„æ•°æ®åº“é›†æˆ
    algo_success, algo_result = await test_individual_algorithms_with_real_data()
    if not algo_success:
        all_success = False
    
    # 3. æµ‹è¯•å®Œæ•´ç®¡é“çš„çœŸå®æ•°æ®å¤„ç†
    pipeline_success, pipeline_result = await test_pipeline_with_real_data()
    if not pipeline_success:
        all_success = False
    
    print("\n" + "=" * 60)
    if all_success:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®åº“é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        print("\nâœ… éªŒè¯ç»“æœ:")
        print("   âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
        print("   âœ… å„ç®—æ³•æ¨¡å—é›†æˆæ­£ç¡®")
        print("   âœ… å®Œæ•´ç®¡é“å¯ç”¨çœŸå®æ•°æ®")
        print("   âœ… å–‚ä¸æœºå’Œå·åŒ…æœºå·¥å•éƒ½èƒ½æ­£ç¡®ç”Ÿæˆ")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“é›†æˆå®ç°")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
