"""
APSæ™ºæ…§æ’äº§ç³»ç»Ÿ - æ–‡ä»¶ä¸Šä¼ API

å®ç°Excelæ–‡ä»¶ä¸Šä¼ ã€è§£æå’ŒéªŒè¯åŠŸèƒ½
æ”¯æŒå¼‚æ­¥æ–‡ä»¶å¤„ç†å’Œè¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š
"""
import os
import uuid
from datetime import datetime
from typing import Optional
from fastapi import APIRouter, UploadFile, File, HTTPException, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import settings
from app.db.connection import get_async_session
from app.schemas.base import (
    FileUploadResponse, ImportBatchInfo, ParseRequest, ParseResponse,
    SuccessResponse, ErrorResponse
)
from app.services.excel_parser import parse_production_plan_excel, ExcelParseError
from app.models.base_models import ImportPlan, DecadePlan

router = APIRouter(prefix="/plans", tags=["è®¡åˆ’æ–‡ä»¶ç®¡ç†"])


async def validate_excel_file(file: UploadFile) -> None:
    """éªŒè¯Excelæ–‡ä»¶"""
    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    if not file.filename:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
    
    file_extension = os.path.splitext(file.filename)[1].lower()
    if file_extension not in settings.upload_allowed_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{file_extension}ï¼Œæ”¯æŒçš„æ ¼å¼ï¼š{settings.upload_allowed_extensions}"
        )
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    if file.size and file.size > settings.upload_max_size:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼š{file.size}å­—èŠ‚ï¼Œæœ€å¤§å…è®¸ï¼š{settings.upload_max_size}å­—èŠ‚"
        )


async def check_filename_uniqueness(
    db: AsyncSession, 
    filename: str, 
    allow_overwrite: bool = False
) -> Optional[ImportPlan]:
    """
    æ£€æŸ¥æ–‡ä»¶åå”¯ä¸€æ€§
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        filename: è¦æ£€æŸ¥çš„æ–‡ä»¶å
        allow_overwrite: æ˜¯å¦å…è®¸è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
        
    Returns:
        å¦‚æœéœ€è¦è¦†ç›–ï¼Œè¿”å›å·²å­˜åœ¨çš„ImportPlanè®°å½•ï¼›å¦åˆ™è¿”å›None
        
    Raises:
        HTTPException: å½“æ–‡ä»¶åé‡å¤ä¸”ä¸å…è®¸è¦†ç›–æ—¶
    """
    from sqlalchemy import select, desc
    
    # æŸ¥è¯¢æ˜¯å¦å­˜åœ¨ç›¸åŒæ–‡ä»¶åçš„è®°å½•
    result = await db.execute(
        select(ImportPlan)
        .where(ImportPlan.file_name == filename)
        .order_by(desc(ImportPlan.created_time))
    )
    existing_plan = result.scalar_one_or_none()
    
    if existing_plan:
        # å¦‚æœä¸å…è®¸è¦†ç›–ï¼Œç›´æ¥æŠ¥é”™
        if not allow_overwrite:
            raise HTTPException(
                status_code=400,
                detail=f"æ–‡ä»¶å '{filename}' å·²å­˜åœ¨ï¼Œä¸Šä¼ æ—¶é—´ï¼š{existing_plan.created_time.strftime('%Y-%m-%d %H:%M:%S')}"
            )
        
        # å¦‚æœå…è®¸è¦†ç›–ï¼Œè¿”å›å·²å­˜åœ¨çš„è®°å½•ç”¨äºåç»­å¤„ç†
        return existing_plan
    
    return None


async def save_uploaded_file(file: UploadFile) -> str:
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    file_extension = os.path.splitext(file.filename)[1]
    unique_filename = f"{uuid.uuid4()}{file_extension}"
    file_path = os.path.join(settings.upload_temp_dir, unique_filename)
    
    # ä¿å­˜æ–‡ä»¶
    try:
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        return file_path
    except Exception as e:
        # æ¸…ç†å¤±è´¥çš„æ–‡ä»¶
        if os.path.exists(file_path):
            os.unlink(file_path)
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")


async def create_import_record(
    db: AsyncSession, 
    import_batch_id: str,
    file_name: str,
    file_path: str,
    file_size: int
) -> ImportPlan:
    """åˆ›å»ºå¯¼å…¥è®°å½•"""
    import_plan = ImportPlan(
        import_batch_id=import_batch_id,
        file_name=file_name,
        file_path=file_path,
        file_size=file_size,
        import_status="UPLOADING",
        import_start_time=datetime.now(),
        created_by="api_user"
    )
    
    db.add(import_plan)
    await db.commit()
    await db.refresh(import_plan)
    return import_plan


@router.post("/upload", response_model=FileUploadResponse)
async def upload_excel_file(
    file: UploadFile = File(..., description="Excelæ–‡ä»¶"),
    allow_overwrite: bool = False,
    db: AsyncSession = Depends(get_async_session)
):
    """
    ä¸Šä¼ Excelæ–‡ä»¶æ¥å£
    
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š.xlsx, .xls
    æœ€å¤§æ–‡ä»¶å¤§å°ï¼š50MB
    
    Args:
        file: Excelæ–‡ä»¶
        allow_overwrite: æ˜¯å¦å…è®¸è¦†ç›–å·²å­˜åœ¨çš„åŒåæ–‡ä»¶ï¼ˆé»˜è®¤Falseï¼‰
        db: æ•°æ®åº“ä¼šè¯
    """
    try:
        # éªŒè¯æ–‡ä»¶
        await validate_excel_file(file)
        
        # æ£€æŸ¥æ–‡ä»¶åå”¯ä¸€æ€§
        existing_plan = await check_filename_uniqueness(db, file.filename, allow_overwrite)
        
        # ç”Ÿæˆå¯¼å…¥æ‰¹æ¬¡ID
        import_batch_id = f"IMPORT_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        # å¦‚æœéœ€è¦è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œå…ˆå¤„ç†æ—§è®°å½•
        if existing_plan:
            # åˆ é™¤æ—§çš„æ–‡ä»¶
            if existing_plan.file_path and os.path.exists(existing_plan.file_path):
                try:
                    os.unlink(existing_plan.file_path)
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤æ—§æ–‡ä»¶å¤±è´¥: {e}")
            
            # åˆ é™¤æ—§çš„decade_planè®°å½•
            from sqlalchemy import delete
            await db.execute(
                delete(DecadePlan).where(DecadePlan.import_batch_id == existing_plan.import_batch_id)
            )
            
            # åˆ é™¤æ—§çš„import_planè®°å½•
            await db.execute(
                delete(ImportPlan).where(ImportPlan.id == existing_plan.id)
            )
            await db.commit()
        
        # ä¿å­˜æ–‡ä»¶
        file_path = await save_uploaded_file(file)
        
        # åˆ›å»ºå¯¼å…¥è®°å½•
        import_plan = await create_import_record(
            db=db,
            import_batch_id=import_batch_id,
            file_name=file.filename,
            file_path=file_path,
            file_size=file.size or 0
        )
        
        # å‡†å¤‡å“åº”æ•°æ®
        upload_info = ImportBatchInfo(
            import_batch_id=import_batch_id,
            file_name=file.filename,
            file_size=file.size or 0,
            upload_time=import_plan.created_time
        )
        
        # æ ¹æ®æ˜¯å¦è¦†ç›–æ–‡ä»¶ç”Ÿæˆä¸åŒçš„å“åº”æ¶ˆæ¯
        message = "æ–‡ä»¶è¦†ç›–ä¸Šä¼ æˆåŠŸ" if existing_plan else "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ"
        
        return FileUploadResponse(
            code=200,
            message=message,
            data=upload_info.model_dump()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼š{str(e)}")


async def parse_excel_background(
    import_batch_id: str,
    file_path: str,
    db: AsyncSession
):
    """åå°è§£æExcelæ–‡ä»¶"""
    try:
        # æ›´æ–°çŠ¶æ€ä¸ºè§£æä¸­
        from sqlalchemy import update
        await db.execute(
            update(ImportPlan)
            .where(ImportPlan.import_batch_id == import_batch_id)
            .values(
                import_status="PARSING",
                import_start_time=datetime.now()
            )
        )
        await db.commit()
        
        # è§£æExcelæ–‡ä»¶
        parse_result = parse_production_plan_excel(file_path)
        
        # æ›´æ–°å¯¼å…¥è®°å½•
        await db.execute(
            update(ImportPlan)
            .where(ImportPlan.import_batch_id == import_batch_id)
            .values(
                total_records=parse_result['total_records'],
                valid_records=parse_result['valid_records'],
                error_records=parse_result['error_records'],
                import_status="COMPLETED",
                import_end_time=datetime.now()
            )
        )
        await db.commit()
        
        # TODO: å°†è§£æç»“æœä¿å­˜åˆ°æ•°æ®åº“ä¸­çš„è¯¦ç»†è¡¨
        # è¿™é‡Œå¯ä»¥å°† parse_result['records'] ä¿å­˜åˆ° aps_decade_plan è¡¨
        
    except Exception as e:
        # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
        await db.execute(
            update(ImportPlan)
            .where(ImportPlan.import_batch_id == import_batch_id)
            .values(
                import_status="FAILED",
                import_end_time=datetime.now(),
                error_message=str(e)
            )
        )
        await db.commit()


@router.post("/{import_batch_id}/parse", response_model=ParseResponse)
async def parse_excel_file(
    import_batch_id: str,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_async_session),
    force_reparse: bool = False
):
    """
    è§£æExcelæ–‡ä»¶æ¥å£
    
    æ”¯æŒåå°å¼‚æ­¥è§£æï¼Œè¿”å›è§£æä»»åŠ¡ä¿¡æ¯
    """
    try:
        # æŸ¥æ‰¾å¯¼å…¥è®°å½•
        from sqlalchemy import select
        result = await db.execute(
            select(ImportPlan).where(ImportPlan.import_batch_id == import_batch_id)
        )
        import_plan = result.scalar_one_or_none()
        
        if not import_plan:
            raise HTTPException(status_code=404, detail=f"å¯¼å…¥æ‰¹æ¬¡ä¸å­˜åœ¨ï¼š{import_batch_id}")
        
        if not os.path.exists(import_plan.file_path):
            raise HTTPException(status_code=404, detail="æºæ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»è§£æè¿‡
        if import_plan.import_status == "COMPLETED" and not force_reparse:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å·²ç»è§£æå®Œæˆï¼Œå¦‚éœ€é‡æ–°è§£æè¯·è®¾ç½®force_reparse=true")
        
        # å¦‚æœæ­£åœ¨è§£æä¸­
        if import_plan.import_status == "PARSING":
            return ParseResponse(
                code=202,
                message="æ–‡ä»¶æ­£åœ¨è§£æä¸­ï¼Œè¯·ç¨åæŸ¥è¯¢ç»“æœ",
                data={
                    "import_batch_id": import_batch_id,
                    "status": "PARSING",
                    "message": "è§£æè¿›è¡Œä¸­..."
                }
            )
        
        # ç›´æ¥è§£æï¼ˆåŒæ­¥æ–¹å¼ï¼Œé€‚åˆå°æ–‡ä»¶ï¼‰
        try:
            parse_result = parse_production_plan_excel(import_plan.file_path)
            
            # æ›´æ–°å¯¼å…¥è®°å½•
            from sqlalchemy import update
            await db.execute(
                update(ImportPlan)
                .where(ImportPlan.import_batch_id == import_batch_id)
                .values(
                    total_records=parse_result['total_records'],
                    valid_records=parse_result['valid_records'],
                    error_records=parse_result['error_records'],
                    import_status="COMPLETED",
                    import_end_time=datetime.now()
                )
            )
            await db.commit()
            
            # ä¿å­˜è§£æç»“æœåˆ°aps_decade_planè¡¨
            await save_parse_results_to_decade_plan(db, import_batch_id, parse_result)
            
            # è½¬æ¢è§£æç»“æœä¸ºå“åº”æ ¼å¼
            from app.schemas.base import ParseResult, ParseResultRecord, ParseErrorInfo
            
            records = []
            for record_data in parse_result['records']:
                record = ParseResultRecord(**record_data)
                records.append(record)
            
            errors = []
            for error_data in parse_result['errors']:
                error = ParseErrorInfo(**error_data)
                errors.append(error)
            
            warnings = []
            for warning_data in parse_result['warnings']:
                warning = ParseErrorInfo(**warning_data)
                warnings.append(warning)
            
            result_data = ParseResult(
                import_batch_id=import_batch_id,
                total_records=parse_result['total_records'],
                valid_records=parse_result['valid_records'],
                error_records=parse_result['error_records'],
                warning_records=parse_result['warning_records'],
                records=records,
                errors=errors,
                warnings=warnings
            )
            
            return ParseResponse(
                code=200,
                message="æ–‡ä»¶è§£ææˆåŠŸ",
                data=result_data
            )
            
        except ExcelParseError as e:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            from sqlalchemy import update
            await db.execute(
                update(ImportPlan)
                .where(ImportPlan.import_batch_id == import_batch_id)
                .values(
                    import_status="FAILED",
                    import_end_time=datetime.now(),
                    error_message=str(e)
                )
            )
            await db.commit()
            
            raise HTTPException(status_code=400, detail=f"Excelè§£æå¤±è´¥ï¼š{str(e)}")
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è§£æè¯·æ±‚å¤„ç†å¤±è´¥ï¼š{str(e)}")


@router.get("/{import_batch_id}/status")
async def get_parse_status(
    import_batch_id: str,
    db: AsyncSession = Depends(get_async_session)
):
    """
    æŸ¥è¯¢è§£æçŠ¶æ€æ¥å£
    """
    try:
        from sqlalchemy import select
        result = await db.execute(
            select(ImportPlan).where(ImportPlan.import_batch_id == import_batch_id)
        )
        import_plan = result.scalar_one_or_none()
        
        if not import_plan:
            raise HTTPException(status_code=404, detail=f"å¯¼å…¥æ‰¹æ¬¡ä¸å­˜åœ¨ï¼š{import_batch_id}")
        
        return SuccessResponse(
            code=200,
            message="æŸ¥è¯¢æˆåŠŸ",
            data={
                "import_batch_id": import_batch_id,
                "file_name": import_plan.file_name,
                "import_status": import_plan.import_status,
                "total_records": import_plan.total_records,
                "valid_records": import_plan.valid_records,
                "error_records": import_plan.error_records,
                "import_start_time": import_plan.import_start_time.isoformat() if import_plan.import_start_time else None,
                "import_end_time": import_plan.import_end_time.isoformat() if import_plan.import_end_time else None,
                "error_message": import_plan.error_message,
                "created_time": import_plan.created_time.isoformat()
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"çŠ¶æ€æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}")


async def save_parse_results_to_decade_plan(db: AsyncSession, import_batch_id: str, parse_result: dict):
    """
    å°†è§£æç»“æœä¿å­˜åˆ°aps_decade_planè¡¨ï¼Œä½¿ç”¨é€—å·åˆ†éš”çš„æœºå°ä»£ç å­—ç¬¦ä¸²
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        import_batch_id: å¯¼å…¥æ‰¹æ¬¡ID
        parse_result: è§£æç»“æœå­—å…¸
    """
    try:
        # å…ˆåˆ é™¤åŒä¸€æ‰¹æ¬¡çš„æ—§æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        from sqlalchemy import delete
        await db.execute(
            delete(DecadePlan).where(DecadePlan.import_batch_id == import_batch_id)
        )
        
        # æ‰¹é‡æ’å…¥æ–°æ•°æ®
        decade_plans = []
        
        # è·å–ä»Excelä¸­æå–çš„å¹´ä»½
        extracted_year = parse_result.get('extracted_year')
        print(f"ğŸ“… ä»è§£æç»“æœè·å–åˆ°çš„å¹´ä»½: {extracted_year}")
        
        # å¤„ç†å¤šå·¥ä½œè¡¨çš„æƒ…å†µ
        if 'sheet_details' in parse_result and parse_result['sheet_details']:
            # å¤šå·¥ä½œè¡¨çš„ç»“æœ
            for sheet_detail in parse_result['sheet_details']:
                for record_data in sheet_detail['records']:
                    # å¦‚æœè®°å½•ä¸­æ²¡æœ‰å¹´ä»½ä¿¡æ¯ï¼Œä½¿ç”¨è§£æç»“æœä¸­çš„å¹´ä»½
                    if 'extracted_year' not in record_data and extracted_year:
                        record_data['extracted_year'] = extracted_year
                    decade_plan = create_decade_plan_record(import_batch_id, record_data)
                    decade_plans.append(decade_plan)
        else:
            # å•å·¥ä½œè¡¨çš„ç»“æœ
            for record_data in parse_result['records']:
                # å¦‚æœè®°å½•ä¸­æ²¡æœ‰å¹´ä»½ä¿¡æ¯ï¼Œä½¿ç”¨è§£æç»“æœä¸­çš„å¹´ä»½
                if 'extracted_year' not in record_data and extracted_year:
                    record_data['extracted_year'] = extracted_year
                decade_plan = create_decade_plan_record(import_batch_id, record_data)
                decade_plans.append(decade_plan)
        
        # æ‰¹é‡æ’å…¥
        if decade_plans:
            db.add_all(decade_plans)
            await db.commit()
            
        print(f"âœ… æˆåŠŸä¿å­˜ {len(decade_plans)} æ¡æ—¬è®¡åˆ’è®°å½•åˆ°æ•°æ®åº“")
        
    except Exception as e:
        await db.rollback()
        print(f"âŒ ä¿å­˜æ—¬è®¡åˆ’æ•°æ®å¤±è´¥: {str(e)}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“è§£ææµç¨‹
        

def create_decade_plan_record(import_batch_id: str, record_data: dict) -> DecadePlan:
    """
    åˆ›å»ºæ—¬è®¡åˆ’è®°å½•å¯¹è±¡ï¼Œé€‚é…æ ‡å‡†è¡¨ç»“æ„
    
    Args:
        import_batch_id: å¯¼å…¥æ‰¹æ¬¡ID
        record_data: è®°å½•æ•°æ®å­—å…¸
        
    Returns:
        DecadePlan: æ—¬è®¡åˆ’è®°å½•å¯¹è±¡
    """
    from datetime import datetime
    import uuid
    import re
    
    # è§£ææ—¥æœŸ - ä¼˜å…ˆä»production_date_rangeæ‹†è§£
    planned_start = None
    planned_end = None
    year = None
    
    # è·å–å¹´ä»½ - ä¼˜å…ˆä»Excelè§£æå™¨æä¾›çš„planned_start/planned_endæ•°æ®ä¸­è·å–
    print(f"ğŸ” æ£€æŸ¥è®°å½•æ•°æ®: planned_start={record_data.get('planned_start')}, planned_end={record_data.get('planned_end')}")
    
    if record_data.get('planned_start'):
        try:
            original_planned_start = datetime.fromisoformat(record_data['planned_start'])
            year = original_planned_start.year
            print(f"âœ… ä»planned_startè·å–å¹´ä»½: {year}")
        except Exception as e:
            print(f"âŒ è§£æplanned_startå¤±è´¥: {e}")
    
    if not year and record_data.get('planned_end'):
        try:
            original_planned_end = datetime.fromisoformat(record_data['planned_end'])
            year = original_planned_end.year
            print(f"âœ… ä»planned_endè·å–å¹´ä»½: {year}")
        except Exception as e:
            print(f"âŒ è§£æplanned_endå¤±è´¥: {e}")
    
    # å¦‚æœè¿˜æ²¡æœ‰å¹´ä»½ï¼Œå°è¯•ä»å…¶ä»–é€”å¾„è·å–
    if not year:
        # æ–¹æ³•1ï¼šæ£€æŸ¥Excelè§£æå™¨æ˜¯å¦æä¾›äº†å¹´ä»½ä¿¡æ¯
        if 'extracted_year' in record_data and record_data['extracted_year']:
            year = record_data['extracted_year']
            print(f"âœ… ä»Excelè§£æå™¨è·å–å¹´ä»½: {year}")
        # æ–¹æ³•2ï¼šå°è¯•ä»æ—¥æœŸèŒƒå›´å­—ç¬¦ä¸²ä¸­æå–å¹´ä»½
        elif record_data.get('production_date_range'):
            date_range = record_data['production_date_range']
            # æŸ¥æ‰¾å¯èƒ½çš„å¹´ä»½æ¨¡å¼ï¼Œå¦‚"2024.10.16-10.31"
            year_match = re.search(r'(\d{4})', str(date_range))
            if year_match:
                year = int(year_match.group(1))
                print(f"âœ… ä»æ—¥æœŸèŒƒå›´æå–å¹´ä»½: {year}")
        
        # æœ€åå¤‡é€‰ï¼šä½¿ç”¨2024å¹´ï¼ˆæ ¹æ®Excelæ ‡é¢˜æ˜¾ç¤ºçš„å¹´ä»½ï¼‰
        if not year:
            year = 2024  # æ ¹æ®Excelæ ‡é¢˜"2024å¹´10æœˆ16ï½31æ—¥ç”Ÿäº§ä½œä¸šè®¡åˆ’è¡¨"
            print(f"âš ï¸ ä½¿ç”¨é»˜è®¤å¹´ä»½2024ï¼ˆä»Excelæ ‡é¢˜è·å¾—ï¼‰: {year}")
    
    # ä»production_date_rangeè§£ææ—¥æœŸèŒƒå›´
    production_date_range = record_data.get('production_date_range', '')
    if production_date_range and '-' in production_date_range:
        try:
            # è§£ææ ¼å¼å¦‚ "10.16-10.31" æˆ– "10.16 - 10.31"
            date_parts = production_date_range.strip().split('-')
            if len(date_parts) == 2:
                start_str = date_parts[0].strip()
                end_str = date_parts[1].strip()
                
                # è§£æå¼€å§‹æ—¥æœŸ "10.16"
                if '.' in start_str:
                    start_month, start_day = start_str.split('.')
                    planned_start = datetime(year, int(start_month), int(start_day))
                    print(f"âœ… ä»production_date_rangeæ„å»ºplanned_start: {planned_start}")
                
                # è§£æç»“æŸæ—¥æœŸ "10.31"
                if '.' in end_str:
                    end_month, end_day = end_str.split('.')
                    planned_end = datetime(year, int(end_month), int(end_day))
                    print(f"âœ… ä»production_date_rangeæ„å»ºplanned_end: {planned_end}")
                
        except (ValueError, IndexError) as e:
            print(f"âš ï¸ è§£æproduction_date_rangeå¤±è´¥: {production_date_range}, é”™è¯¯: {e}")
    
    # å¦‚æœproduction_date_rangeè§£æå¤±è´¥ï¼Œå°è¯•ä»åŸå§‹å­—æ®µè§£æ
    if not planned_start and record_data.get('planned_start'):
        try:
            planned_start = datetime.fromisoformat(record_data['planned_start'])
        except:
            pass
    if not planned_end and record_data.get('planned_end'):
        try:
            planned_end = datetime.fromisoformat(record_data['planned_end'])
        except:
            pass
    
    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ—¥æœŸï¼Œä½¿ç”¨é»˜è®¤æ—¥æœŸ
    if not planned_start:
        planned_start = datetime(year, 11, 1)  # ä½¿ç”¨è§£æå‡ºçš„å¹´ä»½ï¼Œé»˜è®¤11æœˆ1æ—¥
        print(f"âš ï¸ ä½¿ç”¨é»˜è®¤planned_start: {planned_start}")
    if not planned_end:
        planned_end = datetime(year, 11, 15)  # ä½¿ç”¨è§£æå‡ºçš„å¹´ä»½ï¼Œé»˜è®¤11æœˆ15æ—¥
        print(f"âš ï¸ ä½¿ç”¨é»˜è®¤planned_end: {planned_end}")
    
    print(f"ğŸ¯ æœ€ç»ˆæ—¥æœŸç»“æœ: planned_start={planned_start}, planned_end={planned_end}, å¹´ä»½={year}")
    
    # è·å–æœºå°ä»£ç å¹¶è½¬æ¢ä¸ºé€—å·åˆ†éš”å­—ç¬¦ä¸²
    feeder_codes = record_data.get('feeder_codes', [])
    maker_codes = record_data.get('maker_codes', [])
    
    # è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²æ ¼å¼ï¼ˆæ— ç©ºæ ¼ï¼‰
    feeder_code = ','.join(feeder_codes) if feeder_codes else 'UNKNOWN'
    maker_code = ','.join(maker_codes) if maker_codes else 'UNKNOWN'
    
    # ç”Ÿæˆå·¥ä½œè®¢å•å·ï¼ˆä½¿ç”¨è¡Œå·å³å¯ï¼Œä¸éœ€è¦æœºå°åç¼€ï¼‰
    row_number = record_data.get('row_number', 0)
    work_order_nr = f"WO_{row_number}"
    
    # è·å–æ•°é‡ï¼Œç¡®ä¿ä¸ä¸ºNone
    quantity_total = record_data.get('material_input') or 0
    final_quantity = record_data.get('final_quantity') or 0
    
    # è·å–ç‰Œå·ï¼Œä½¿ç”¨article_nameä½œä¸ºarticle_nr
    article_name = record_data.get('article_name') or 'UNKNOWN'
    article_nr = record_data.get('article_nr') or article_name
    
    # ç¡®å®šéªŒè¯çŠ¶æ€
    validation_status = 'VALID'
    validation_message = None
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    if not record_data.get('article_name'):
        validation_status = 'ERROR'
        validation_message = 'ç¼ºå°‘ç‰Œå·ä¿¡æ¯'
    elif feeder_code == 'UNKNOWN' and maker_code == 'UNKNOWN':
        validation_status = 'ERROR'
        validation_message = 'ç¼ºå°‘æœºå°ä¿¡æ¯'
    # ç§»é™¤æ•°é‡æ£€æŸ¥ - å…è®¸ç»§æ‰¿çš„ç©ºå€¼
    
    return DecadePlan(
        import_batch_id=import_batch_id,
        work_order_nr=work_order_nr,
        article_nr=article_nr,
        package_type=record_data.get('package_type'),
        specification=record_data.get('specification'),
        quantity_total=quantity_total,
        final_quantity=final_quantity,
        production_unit=record_data.get('production_unit'),
        maker_code=maker_code,
        feeder_code=feeder_code,
        planned_start=planned_start,
        planned_end=planned_end,
        production_date_range=record_data.get('production_date_range'),
        row_number=record_data.get('row_number'),
        validation_status=validation_status,
        validation_message=validation_message
    )


@router.get("/history")
async def get_upload_history(
    page: int = 1,
    page_size: int = 20,
    status: Optional[str] = None,
    scheduling_status: Optional[str] = None,  # æ–°å¢ï¼šæ’äº§çŠ¶æ€è¿‡æ»¤
    db: AsyncSession = Depends(get_async_session)
):
    """
    è·å–ä¸Šä¼ å†å²è®°å½•ï¼ŒåŒ…å«æ’äº§çŠ¶æ€ä¿¡æ¯
    
    Args:
        page: é¡µç ï¼Œä»1å¼€å§‹
        page_size: æ¯é¡µå¤§å°ï¼Œé»˜è®¤20
        status: è¿‡æ»¤çŠ¶æ€ï¼Œå¯é€‰å€¼ï¼šUPLOADING, PARSING, COMPLETED, FAILED
        scheduling_status: æ’äº§çŠ¶æ€è¿‡æ»¤
            - unscheduled: æœªæ’äº§
            - scheduling: æ’äº§ä¸­  
            - completed: æ’äº§å®Œæˆ
            - failed: æ’äº§å¤±è´¥
        db: æ•°æ®åº“ä¼šè¯
    """
    try:
        from sqlalchemy import select, and_, desc, func, outerjoin
        from app.models.scheduling_models import SchedulingTask, SchedulingTaskStatus
        
        # æ„å»ºè”åˆæŸ¥è¯¢ï¼šImportPlan + SchedulingTask
        base_query = select(
            ImportPlan,
            SchedulingTask.task_id,
            SchedulingTask.task_status,
            SchedulingTask.result_summary
        ).outerjoin(
            SchedulingTask, 
            ImportPlan.import_batch_id == SchedulingTask.import_batch_id
        )
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = []
        if status:
            conditions.append(ImportPlan.import_status == status)
            
        # æ’äº§çŠ¶æ€è¿‡æ»¤
        if scheduling_status == 'unscheduled':
            conditions.append(SchedulingTask.task_id == None)
        elif scheduling_status == 'scheduling':
            conditions.append(SchedulingTask.task_status == SchedulingTaskStatus.RUNNING)
        elif scheduling_status == 'completed':
            conditions.append(SchedulingTask.task_status == SchedulingTaskStatus.COMPLETED)
        elif scheduling_status == 'failed':
            conditions.append(SchedulingTask.task_status == SchedulingTaskStatus.FAILED)
        
        if conditions:
            base_query = base_query.where(and_(*conditions))
        
        # è·å–æ€»æ•°
        count_query = select(func.count()).select_from(ImportPlan)
        if conditions:
            count_query = count_query.outerjoin(
                SchedulingTask, 
                ImportPlan.import_batch_id == SchedulingTask.import_batch_id
            ).where(and_(*conditions))
        
        count_result = await db.execute(count_query)
        total_count = count_result.scalar()
        
        # åˆ†é¡µæŸ¥è¯¢
        offset = (page - 1) * page_size
        query = base_query.order_by(desc(ImportPlan.created_time)).offset(offset).limit(page_size)
        
        result = await db.execute(query)
        records_with_tasks = result.fetchall()
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        records = []
        for plan, task_id, task_status, result_summary in records_with_tasks:
            # ç¡®å®šæ’äº§çŠ¶æ€
            if not task_id:
                scheduling_status_value = 'unscheduled'
                scheduling_text = 'æœªæ’äº§'
            else:
                scheduling_status_value = task_status.value.lower()
                scheduling_text = {
                    'pending': 'å¾…æ’äº§',
                    'running': 'æ’äº§ä¸­',
                    'completed': 'å·²å®Œæˆ',
                    'failed': 'æ’äº§å¤±è´¥',
                    'cancelled': 'å·²å–æ¶ˆ'
                }.get(task_status.value.lower(), task_status.value)
            
            records.append({
                "batch_id": plan.import_batch_id,
                "file_name": plan.file_name,
                "file_size": plan.file_size,
                "upload_time": plan.created_time.isoformat(),
                "import_start_time": plan.import_start_time.isoformat() if plan.import_start_time else None,
                "import_end_time": plan.import_end_time.isoformat() if plan.import_end_time else None,
                "status": plan.import_status,
                "total_records": plan.total_records,
                "valid_records": plan.valid_records,
                "error_records": plan.error_records,
                "error_message": plan.error_message,
                
                # æ–°å¢æ’äº§ç›¸å…³ä¿¡æ¯
                "task_id": task_id,
                "scheduling_status": scheduling_status_value,
                "scheduling_text": scheduling_text,
                "work_orders_summary": result_summary.get('total_work_orders', 0) if result_summary else 0,
                "can_schedule": (plan.import_status == 'COMPLETED' and 
                               not task_id and 
                               plan.valid_records > 0),  # å·²è§£æã€æœªæ’äº§ä¸”æœ‰æœ‰æ•ˆè®°å½•
            })
        
        # è®¡ç®—åˆ†é¡µä¿¡æ¯
        total_pages = (total_count + page_size - 1) // page_size
        
        return SuccessResponse(
            code=200,
            message="æŸ¥è¯¢æˆåŠŸ",
            data={
                "records": records,
                "pagination": {
                    "page": page,
                    "page_size": page_size,
                    "total_count": total_count,
                    "total_pages": total_pages,
                    "has_next": page < total_pages,
                    "has_prev": page > 1
                }
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å†å²è®°å½•å¤±è´¥ï¼š{str(e)}")


@router.get("/statistics")
async def get_upload_statistics(
    db: AsyncSession = Depends(get_async_session)
):
    """
    è·å–ä¸Šä¼ ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        from sqlalchemy import select, func, and_
        from datetime import datetime, timedelta
        
        # ä»Šæ—¥ç»Ÿè®¡
        today = datetime.now().date()
        today_start = datetime.combine(today, datetime.min.time())
        today_end = datetime.combine(today, datetime.max.time())
        
        # æœ¬æœˆç»Ÿè®¡
        month_start = today.replace(day=1)
        month_start_dt = datetime.combine(month_start, datetime.min.time())
        
        # ä»Šæ—¥ä¸Šä¼ æ•°é‡
        today_uploads_result = await db.execute(
            select(func.count()).select_from(ImportPlan)
            .where(and_(
                ImportPlan.created_time >= today_start,
                ImportPlan.created_time <= today_end
            ))
        )
        today_uploads = today_uploads_result.scalar() or 0
        
        # æœ¬æœˆå¤„ç†è®°å½•æ•°
        monthly_records_result = await db.execute(
            select(func.sum(ImportPlan.total_records)).select_from(ImportPlan)
            .where(and_(
                ImportPlan.created_time >= month_start_dt,
                ImportPlan.import_status == 'COMPLETED'
            ))
        )
        monthly_processed = monthly_records_result.scalar() or 0
        
        # æˆåŠŸç‡è®¡ç®—ï¼ˆæœ€è¿‘30å¤©ï¼‰
        thirty_days_ago = datetime.now() - timedelta(days=30)
        total_recent_result = await db.execute(
            select(func.count()).select_from(ImportPlan)
            .where(ImportPlan.created_time >= thirty_days_ago)
        )
        total_recent = total_recent_result.scalar() or 0
        
        success_recent_result = await db.execute(
            select(func.count()).select_from(ImportPlan)
            .where(and_(
                ImportPlan.created_time >= thirty_days_ago,
                ImportPlan.import_status == 'COMPLETED'
            ))
        )
        success_recent = success_recent_result.scalar() or 0
        
        success_rate = round((success_recent / total_recent * 100), 1) if total_recent > 0 else 0
        
        # æ´»è·ƒæ‰¹æ¬¡ï¼ˆè§£æä¸­æˆ–æœ€è¿‘å®Œæˆçš„ï¼‰
        active_batches_result = await db.execute(
            select(func.count()).select_from(ImportPlan)
            .where(ImportPlan.import_status.in_(['PARSING', 'UPLOADING']))
        )
        active_batches = active_batches_result.scalar() or 0
        
        return SuccessResponse(
            code=200,
            message="ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ",
            data={
                "today_uploads": today_uploads,
                "monthly_processed": monthly_processed,
                "success_rate": success_rate,
                "active_batches": active_batches
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ï¼š{str(e)}")


@router.get("/scheduling-statistics")
async def get_scheduling_statistics(
    db: AsyncSession = Depends(get_async_session)
):
    """
    è·å–æ’äº§ç›¸å…³çš„å…¨å±€ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        from sqlalchemy import select, func, and_, outerjoin
        from app.models.scheduling_models import SchedulingTask, SchedulingTaskStatus
        
        # æ„å»ºè”åˆæŸ¥è¯¢ï¼šImportPlan + SchedulingTask
        base_query = select(
            ImportPlan,
            SchedulingTask.task_id,
            SchedulingTask.task_status
        ).outerjoin(
            SchedulingTask, 
            ImportPlan.import_batch_id == SchedulingTask.import_batch_id
        ).where(ImportPlan.import_status == 'COMPLETED')  # åªç»Ÿè®¡å·²è§£æå®Œæˆçš„
        
        result = await db.execute(base_query)
        all_plans = result.fetchall()
        
        # ç»Ÿè®¡å„ç§çŠ¶æ€
        available_plans_count = 0  # å¾…æ’äº§è®¡åˆ’
        running_tasks_count = 0    # è¿›è¡Œä¸­
        completed_tasks_count = 0  # å·²å®Œæˆ
        
        for plan, task_id, task_status in all_plans:
            if not task_id:
                # æœªæ’äº§çš„è®°å½•
                if plan.valid_records > 0:  # åªæœ‰æœ‰æœ‰æ•ˆè®°å½•çš„æ‰ç®—å¾…æ’äº§
                    available_plans_count += 1
            else:
                # å·²æ’äº§çš„è®°å½•ï¼Œæ ¹æ®ä»»åŠ¡çŠ¶æ€åˆ†ç±»
                if task_status in [SchedulingTaskStatus.PENDING, SchedulingTaskStatus.RUNNING]:
                    running_tasks_count += 1
                elif task_status == SchedulingTaskStatus.COMPLETED:
                    completed_tasks_count += 1
        
        return SuccessResponse(
            code=200,
            message="æ’äº§ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ",
            data={
                "available_plans_count": available_plans_count,
                "running_tasks_count": running_tasks_count,
                "completed_tasks_count": completed_tasks_count
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ’äº§ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ï¼š{str(e)}")


@router.get("/{import_batch_id}/decade-plans")
async def get_decade_plans(
    import_batch_id: str,
    db: AsyncSession = Depends(get_async_session)
):
    """
    æŸ¥è¯¢æ—¬è®¡åˆ’è®°å½•æ¥å£
    """
    try:
        from sqlalchemy import select
        result = await db.execute(
            select(DecadePlan).where(DecadePlan.import_batch_id == import_batch_id)
        )
        decade_plans = result.scalars().all()
        
        if not decade_plans:
            raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°å¯¼å…¥æ‰¹æ¬¡çš„æ—¬è®¡åˆ’è®°å½•ï¼š{import_batch_id}")
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        plans_data = []
        for plan in decade_plans:
            plans_data.append({
                "work_order_nr": plan.work_order_nr,
                "article_nr": plan.article_nr,
                "package_type": plan.package_type,
                "specification": plan.specification,
                "feeder_code": plan.feeder_code,
                "maker_code": plan.maker_code,
                "quantity_total": plan.quantity_total,
                "final_quantity": plan.final_quantity,
                "planned_start": plan.planned_start.isoformat() if plan.planned_start else None,
                "planned_end": plan.planned_end.isoformat() if plan.planned_end else None,
                "row_number": plan.row_number,
                "validation_status": plan.validation_status,
                "validation_message": plan.validation_message
            })
        
        return SuccessResponse(
            code=200,
            message="æŸ¥è¯¢æˆåŠŸ",
            data={
                "import_batch_id": import_batch_id,
                "total_plans": len(plans_data),
                "plans": plans_data
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢æ—¬è®¡åˆ’å¤±è´¥ï¼š{str(e)}")


@router.get("/available-for-scheduling")
async def get_available_batches_for_scheduling(
    db: AsyncSession = Depends(get_async_session)
):
    """
    è·å–å¯ç”¨äºæ’äº§çš„æ‰¹æ¬¡åˆ—è¡¨
    æ¡ä»¶ï¼šimport_status = 'COMPLETED' ä¸”æ²¡æœ‰å¯¹åº”çš„æ’äº§ä»»åŠ¡
    """
    try:
        from sqlalchemy import select, and_, desc, outerjoin
        from app.models.scheduling_models import SchedulingTask
        
        # æŸ¥è¯¢å·²è§£æå®Œæˆä½†æœªæ’äº§çš„æ‰¹æ¬¡ï¼ˆå¿…é¡»æœ‰æœ‰æ•ˆè®°å½•ï¼‰
        query = select(ImportPlan).outerjoin(
            SchedulingTask, 
            ImportPlan.import_batch_id == SchedulingTask.import_batch_id
        ).where(and_(
            ImportPlan.import_status == 'COMPLETED',
            ImportPlan.valid_records > 0,  # å¿…é¡»æœ‰æœ‰æ•ˆè®°å½•
            SchedulingTask.task_id == None  # æœªæ’äº§
        )).order_by(desc(ImportPlan.created_time))
        
        result = await db.execute(query)
        import_plans = result.scalars().all()
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        available_batches = []
        for plan in import_plans:
            available_batches.append({
                "batch_id": plan.import_batch_id,
                "file_name": plan.file_name,
                "total_records": plan.total_records,
                "valid_records": plan.valid_records,
                "import_end_time": plan.import_end_time.isoformat() if plan.import_end_time else None,
                "display_name": f"{plan.file_name} ({plan.valid_records}æ¡è®°å½•)",
                "can_schedule": True
            })
        
        return SuccessResponse(
            code=200,
            message="æŸ¥è¯¢æˆåŠŸ",
            data={
                "available_batches": available_batches,
                "total_count": len(available_batches)
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¯æ’äº§æ‰¹æ¬¡å¤±è´¥ï¼š{str(e)}")


@router.get("/{import_batch_id}/scheduling-history")
async def get_batch_scheduling_history(
    import_batch_id: str,
    db: AsyncSession = Depends(get_async_session)
):
    """
    è·å–ç‰¹å®šæ‰¹æ¬¡çš„æ‰€æœ‰æ’äº§å†å²è®°å½•
    """
    try:
        from sqlalchemy import select, desc
        from app.models.scheduling_models import SchedulingTask
        
        # æŸ¥è¯¢è¯¥æ‰¹æ¬¡çš„æ‰€æœ‰æ’äº§ä»»åŠ¡
        query = select(SchedulingTask).where(
            SchedulingTask.import_batch_id == import_batch_id
        ).order_by(desc(SchedulingTask.created_time))
        
        result = await db.execute(query)
        tasks = result.scalars().all()
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        history_records = []
        for task in tasks:
            history_records.append({
                "task_id": task.task_id,
                "task_name": task.task_name,
                "status": task.task_status.value,
                "progress": task.progress,
                "start_time": task.start_time.isoformat() if task.start_time else None,
                "end_time": task.end_time.isoformat() if task.end_time else None,
                "execution_duration": task.execution_duration,
                "error_message": task.error_message,
                "result_summary": task.result_summary
            })
        
        return SuccessResponse(
            code=200,
            message="æŸ¥è¯¢æˆåŠŸ",
            data={
                "import_batch_id": import_batch_id,
                "total_tasks": len(history_records),
                "history": history_records
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢æ‰¹æ¬¡æ’äº§å†å²å¤±è´¥ï¼š{str(e)}")