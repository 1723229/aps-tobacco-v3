"""
APSæ™ºæ…§æ’äº§ç³»ç»Ÿ - æ’äº§ç®—æ³•æ‰§è¡ŒAPI

å®ç°æ’äº§ç®—æ³•æ‰§è¡Œã€çŠ¶æ€æŸ¥è¯¢ã€å·¥å•æŸ¥è¯¢ç­‰åŠŸèƒ½
"""
import uuid
from datetime import datetime, date
from typing import Optional, Dict, Any
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.connection import get_async_session
from app.schemas.base import SuccessResponse, ErrorResponse
from app.algorithms.scheduling_engine import SchedulingEngine
from app.algorithms.pipeline import AlgorithmPipeline
from app.models.scheduling_models import SchedulingTask, SchedulingTaskStatus
from app.models.work_order_models import PackingOrder, FeedingOrder
from sqlalchemy import select, func

router = APIRouter(prefix="/scheduling", tags=["æ’äº§ç®—æ³•ç®¡ç†"])


def _parse_datetime(datetime_str_or_obj):
    """è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²æˆ–å¯¹è±¡"""
    if isinstance(datetime_str_or_obj, datetime):
        return datetime_str_or_obj
    elif isinstance(datetime_str_or_obj, str):
        try:
            # å°è¯•è§£æ "2024/10/16 15:40:00" æ ¼å¼
            return datetime.strptime(datetime_str_or_obj, '%Y/%m/%d %H:%M:%S')
        except ValueError:
            try:
                # å°è¯•è§£æ "2024-10-16 15:40:00" æ ¼å¼
                return datetime.strptime(datetime_str_or_obj, '%Y-%m-%d %H:%M:%S')
            except ValueError:
                # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›å½“å‰æ—¶é—´
                return datetime.now()
    else:
        return datetime.now()


def _parse_date(date_str_or_obj):
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²æˆ–å¯¹è±¡"""
    if isinstance(date_str_or_obj, date):
        return date_str_or_obj
    elif isinstance(date_str_or_obj, datetime):
        return date_str_or_obj.date()
    elif isinstance(date_str_or_obj, str):
        try:
            # å°è¯•è§£æ "2024/10/16" æ ¼å¼
            return datetime.strptime(date_str_or_obj, '%Y/%m/%d').date()
        except ValueError:
            try:
                # å°è¯•è§£æ "2024-10-16" æ ¼å¼
                return datetime.strptime(date_str_or_obj, '%Y-%m-%d').date()
            except ValueError:
                # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›å½“å‰æ—¥æœŸ
                return date.today()
    else:
        return date.today()


from pydantic import BaseModel

class SchedulingRequest(BaseModel):
    import_batch_id: str
    algorithm_config: Optional[Dict[str, Any]] = None

@router.post("/execute")
async def execute_scheduling_algorithm(
    request: SchedulingRequest,
    db: AsyncSession = Depends(get_async_session),
    background_tasks: BackgroundTasks = None
):
    """
    æ‰§è¡Œæ’äº§ç®—æ³•æ¥å£ - æ”¹ä¸ºåŒæ­¥æ‰§è¡Œæ–¹ä¾¿æµ‹è¯•
    
    Args:
        request: æ’äº§è¯·æ±‚å‚æ•°
    
    Returns:
        æ’äº§ä»»åŠ¡ä¿¡æ¯å’Œæ‰§è¡Œç»“æœ
    """
    try:
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"SCHEDULE_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        # åˆ›å»ºæ’äº§ä»»åŠ¡è®°å½•
        scheduling_task = SchedulingTask(
            task_id=task_id,
            import_batch_id=request.import_batch_id,
            task_name=f"æ—¬è®¡åˆ’æ’äº§-{request.import_batch_id}",
            task_status=SchedulingTaskStatus.PENDING,
            current_stage="åˆå§‹åŒ–",
            progress=0,
            merge_enabled=request.algorithm_config.get("merge_enabled", True) if request.algorithm_config else True,
            split_enabled=request.algorithm_config.get("split_enabled", True) if request.algorithm_config else True,
            correction_enabled=request.algorithm_config.get("correction_enabled", True) if request.algorithm_config else True,
            parallel_enabled=request.algorithm_config.get("parallel_enabled", True) if request.algorithm_config else True,
            created_by="api_user"
        )
        
        db.add(scheduling_task)
        await db.commit()
        await db.refresh(scheduling_task)

        # å¯åŠ¨åå°ä»»åŠ¡æ‰§è¡Œæ’äº§ç®—æ³•
        if background_tasks:
            background_tasks.add_task(
                execute_scheduling_pipeline_background,
                task_id=task_id,
                import_batch_id=request.import_batch_id,
                algorithm_config=request.algorithm_config or {}
            )

        # è¿”å›ä»»åŠ¡åˆ›å»ºç»“æœ
        return SuccessResponse(
            code=200,
            message="æ’äº§ä»»åŠ¡åˆ›å»ºæˆåŠŸ",
            data={
                "task_id": task_id,
                "import_batch_id": request.import_batch_id,
                "status": "PENDING",
                "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œæ’äº§ç®—æ³•"
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ’äº§ç®—æ³•æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")


async def execute_scheduling_pipeline_background(
    task_id: str,
    import_batch_id: str,
    algorithm_config: Dict[str, Any]
):
    """
    åå°æ‰§è¡Œæ’äº§ç®—æ³•ç®¡é“çš„å®Œæ•´æµç¨‹
    """
    from app.db.connection import get_db_session
    
    async with get_db_session() as db:
        try:
            # è·å–ä»»åŠ¡è®°å½•
            result = await db.execute(
                select(SchedulingTask).where(SchedulingTask.task_id == task_id)
            )
            task = result.scalar_one_or_none()
            
            if not task:
                return
                
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
            task.task_status = SchedulingTaskStatus.RUNNING
            task.start_time = datetime.now()
            task.current_stage = "æ•°æ®é¢„å¤„ç†"
            task.progress = 10
            await db.commit()
            
            # åˆ›å»ºç®—æ³•ç®¡é“å®ä¾‹
            pipeline = AlgorithmPipeline()
            
            # æ‰§è¡Œå®Œæ•´çš„æ’äº§ç®—æ³•ç®¡é“
            pipeline_result = await pipeline.execute_full_pipeline_with_batch(
                import_batch_id=import_batch_id,
                use_real_data=True
            )
            
            if pipeline_result.get('success', False):
                # è·å–ç”Ÿæˆçš„å·¥å•æ•°æ®ï¼ˆç”¨äºaps_packing_orderå’Œaps_feeding_orderï¼‰
                final_work_orders = pipeline_result.get('final_work_orders', [])
                print(f"ğŸ” DEBUG: è·å–åˆ° {len(final_work_orders)} ä¸ªå·¥å•æ•°æ®")
                
                # è·å–åˆå¹¶åçš„è®¡åˆ’æ•°æ®ï¼ˆç”¨äºaps_work_order_scheduleï¼‰
                merged_plans = pipeline_result.get('merged_plans', [])
                print(f"ğŸ” DEBUG: è·å–åˆ° {len(merged_plans)} ä¸ªåˆå¹¶åçš„è®¡åˆ’æ•°æ®")
                
                # æŒä¹…åŒ–å·¥å•åˆ°æ•°æ®åº“ - ä½¿ç”¨ç›´æ¥SQLæ’å…¥é¿å…ORMæ¨¡å‹å†²çª
                packing_orders_count = 0
                feeding_orders_count = 0
                
                # ç¡®ä¿dateåœ¨è¿™é‡Œå¯¼å…¥ï¼Œé¿å…åœ¨æ¡ä»¶åˆ†æ”¯ä¸­å¯¼å…¥å¯¼è‡´çš„ä½œç”¨åŸŸé—®é¢˜
                from datetime import date
                from sqlalchemy import text
                
                print(f"ğŸ” DEBUG: å¼€å§‹å¤„ç†å·¥å•ï¼Œtask_id = {task_id}")
                
                for i, work_order in enumerate(final_work_orders):
                    # å…¼å®¹ä¸åŒçš„å­—æ®µåï¼šorder_type æˆ– work_order_type
                    order_type = work_order.get('order_type') or work_order.get('work_order_type', '')
                    print(f"ğŸ” DEBUG: å¤„ç†å·¥å• {i+1}/{len(final_work_orders)}, order_type = {order_type}")
                    
                    if order_type == 'PACKING':
                        
                        # å¤„ç†æœºå™¨ä»£ç  - ä»ç”Ÿæˆçš„å·¥å•æ•°æ®ä¸­è·å–
                        machine_code_raw = work_order.get('maker_code') or work_order.get('production_line', 'C1')
                        if ',' in machine_code_raw:
                            machine_code_raw = machine_code_raw.split(',')[0].strip()  # å–ç¬¬ä¸€ä¸ªæœºå™¨ä»£ç å¹¶å»æ‰ç©ºæ ¼
                        
                        # æ˜ å°„æœºå™¨ä»£ç åˆ°æ•°æ®åº“æ ¼å¼ - æ•°æ®åº“ä¸­å®é™…æ˜¯C1, C2, C3è€Œä¸æ˜¯C01, C02, C03
                        maker_code = machine_code_raw or 'C1'  # ç›´æ¥ä½¿ç”¨åŸå§‹ä»£ç ï¼Œé»˜è®¤ä½¿ç”¨C1
                        
                        print(f"ğŸ” DEBUG: å‡†å¤‡æ’å…¥å·åŒ…æœºå·¥å•ï¼Œplan_id = {work_order.get('plan_id')}, production_line = {maker_code}")
                        
                        # å·åŒ…æœºå·¥å• - ç›´æ¥SQLæ’å…¥
                        insert_sql = """
                        INSERT INTO aps_packing_order (
                            plan_id, production_line, batch_code, material_code, bom_revision, 
                            quantity, plan_start_time, plan_end_time, sequence, shift,
                            input_plan_id, input_batch_code, input_quantity, batch_sequence,
                            is_whole_batch, is_main_channel, is_deleted, is_last_one,
                            input_material_code, input_bom_revision, tiled,
                            is_vaccum, is_sh93, is_hdt, is_flavor, unit, plan_date,
                            plan_output_quantity, is_outsourcing, is_backup, task_id, order_status,
                            created_time, updated_time
                        ) VALUES (
                            :plan_id, :production_line, :batch_code, :material_code, :bom_revision,
                            :quantity, :plan_start_time, :plan_end_time, :sequence, :shift,
                            :input_plan_id, :input_batch_code, :input_quantity, :batch_sequence,
                            :is_whole_batch, :is_main_channel, :is_deleted, :is_last_one,
                            :input_material_code, :input_bom_revision, :tiled,
                            :is_vaccum, :is_sh93, :is_hdt, :is_flavor, :unit, :plan_date,
                            :plan_output_quantity, :is_outsourcing, :is_backup, :task_id, :order_status,
                            NOW(), NOW()
                        )
                        """
                        
                        try:
                            await db.execute(text(insert_sql), {
                                'plan_id': work_order.get('plan_id') or work_order.get('work_order_nr', f"HJB{work_order.get('original_work_order_nr', '')}"),
                                'production_line': maker_code,
                                'batch_code': work_order.get('batch_code'),
                                'material_code': work_order.get('material_code') or work_order.get('article_nr', 'UNKNOWN'),
                                'bom_revision': work_order.get('bom_revision'),
                                'quantity': work_order.get('quantity') or work_order.get('final_quantity', 0),
                                'plan_start_time': work_order.get('plan_start_time') or work_order.get('planned_start'),
                                'plan_end_time': work_order.get('plan_end_time') or work_order.get('planned_end'),
                                'sequence': work_order.get('sequence', 1),
                                'shift': work_order.get('shift', 'ç™½ç­'),
                                'input_plan_id': work_order.get('input_plan_id'),
                                'input_batch_code': work_order.get('input_batch_code'),
                                'input_quantity': str(work_order.get('input_quantity', 0)),
                                'batch_sequence': work_order.get('batch_sequence', '1'),
                                'is_whole_batch': work_order.get('is_whole_batch', True),
                                'is_main_channel': work_order.get('is_main_channel', True),
                                'is_deleted': work_order.get('is_deleted', False),
                                'is_last_one': work_order.get('is_last_one', False),
                                'input_material_code': work_order.get('input_material_code'),
                                'input_bom_revision': work_order.get('input_bom_revision'),
                                'tiled': work_order.get('tiled', False),
                                'is_vaccum': work_order.get('is_vaccum', False),
                                'is_sh93': work_order.get('is_sh93', False),
                                'is_hdt': work_order.get('is_hdt', False),
                                'is_flavor': work_order.get('is_flavor', False),
                                'unit': work_order.get('unit', 'ç®±'),
                                'plan_date': _parse_date(work_order.get('plan_date', date.today())),
                                'plan_output_quantity': work_order.get('plan_output_quantity'),
                                'is_outsourcing': work_order.get('is_outsourcing', False),
                                'is_backup': work_order.get('is_backup', False),
                                'task_id': task_id,
                                'order_status': 'PLANNED'
                            })
                            packing_orders_count += 1
                            print(f"ğŸ” DEBUG: å·åŒ…æœºå·¥å•æ’å…¥æˆåŠŸ")
                        except Exception as e:
                            print(f"ğŸ” DEBUG: å·åŒ…æœºå·¥å•æ’å…¥å¤±è´¥: {e}")
                            raise
                        
                    elif order_type == 'FEEDING':
                        # å¤„ç†æœºå™¨ä»£ç  - ä»ç”Ÿæˆçš„å·¥å•æ•°æ®ä¸­è·å–
                        machine_code_raw = work_order.get('feeder_code') or work_order.get('production_line', '15')
                        if ',' in machine_code_raw:
                            machine_code_raw = machine_code_raw.split(',')[0].strip()  # å–ç¬¬ä¸€ä¸ªæœºå™¨ä»£ç å¹¶å»æ‰ç©ºæ ¼
                        
                        # æ˜ å°„æœºå™¨ä»£ç åˆ°æ•°æ®åº“æ ¼å¼ - æ•°æ®åº“ä¸­æœ‰F01æˆ–çº¯æ•°å­—å¦‚15,16,17...32
                        feeder_code = machine_code_raw or '15'  # ç›´æ¥ä½¿ç”¨åŸå§‹ä»£ç ï¼Œé»˜è®¤ä½¿ç”¨15
                        
                        print(f"ğŸ” DEBUG: å‡†å¤‡æ’å…¥å–‚ä¸æœºå·¥å•ï¼Œplan_id = {work_order.get('plan_id')}, production_line = {feeder_code}")
                        
                        # å–‚ä¸æœºå·¥å• - ç›´æ¥SQLæ’å…¥
                        insert_sql = """
                        INSERT INTO aps_feeding_order (
                            plan_id, production_line, batch_code, material_code, bom_revision, 
                            quantity, plan_start_time, plan_end_time, sequence, shift,
                            is_vaccum, is_sh93, is_hdt, is_flavor, unit, plan_date,
                            plan_output_quantity, is_outsourcing, is_backup, task_id, order_status,
                            created_time, updated_time
                        ) VALUES (
                            :plan_id, :production_line, :batch_code, :material_code, :bom_revision,
                            :quantity, :plan_start_time, :plan_end_time, :sequence, :shift,
                            :is_vaccum, :is_sh93, :is_hdt, :is_flavor, :unit, :plan_date,
                            :plan_output_quantity, :is_outsourcing, :is_backup, :task_id, :order_status,
                            NOW(), NOW()
                        )
                        """
                        
                        try:
                            await db.execute(text(insert_sql), {
                                'plan_id': work_order.get('plan_id') or work_order.get('work_order_nr', f"HWS{work_order.get('original_work_order_nr', '')}"),
                                'production_line': feeder_code,
                                'batch_code': work_order.get('batch_code'),
                                'material_code': work_order.get('material_code') or work_order.get('article_nr', 'UNKNOWN'),
                                'bom_revision': work_order.get('bom_revision'),
                                'quantity': str(work_order.get('quantity') or work_order.get('final_quantity', 0)),
                                'plan_start_time': work_order.get('plan_start_time') or work_order.get('planned_start'),
                                'plan_end_time': work_order.get('plan_end_time') or work_order.get('planned_end'),
                                'sequence': work_order.get('sequence', 1),
                                'shift': work_order.get('shift', 'ç™½ç­'),
                                'is_vaccum': work_order.get('is_vaccum', False),
                                'is_sh93': work_order.get('is_sh93', False),
                                'is_hdt': work_order.get('is_hdt', False),
                                'is_flavor': work_order.get('is_flavor', False),
                                'unit': work_order.get('unit', 'å…¬æ–¤'),
                                'plan_date': _parse_date(work_order.get('plan_date', date.today())),
                                'plan_output_quantity': work_order.get('plan_output_quantity'),
                                'is_outsourcing': work_order.get('is_outsourcing', False),
                                'is_backup': work_order.get('is_backup', False),
                                'task_id': task_id,
                                'order_status': 'PLANNED'
                            })
                            feeding_orders_count += 1
                            print(f"ğŸ” DEBUG: å–‚ä¸æœºå·¥å•æ’å…¥æˆåŠŸ")
                        except Exception as e:
                            print(f"ğŸ” DEBUG: å–‚ä¸æœºå·¥å•æ’å…¥å¤±è´¥: {e}")
                            raise
                    
                    else:
                        print(f"ğŸ” DEBUG: æœªçŸ¥å·¥å•ç±»å‹: {order_type}")
                
                print(f"ğŸ” DEBUG: å·¥å•å¤„ç†å®Œæˆï¼Œå‡†å¤‡æäº¤äº‹åŠ¡ï¼Œå·åŒ…æœº: {packing_orders_count}, å–‚ä¸æœº: {feeding_orders_count}")
                
                # å†™å…¥å·¥å•è°ƒåº¦æ•°æ®åˆ° aps_work_order_schedule è¡¨ï¼ˆä½¿ç”¨åˆå¹¶åçš„è®¡åˆ’æ•°æ®ï¼‰
                print(f"ğŸ” DEBUG: å¼€å§‹å†™å…¥å·¥å•è°ƒåº¦æ•°æ®åˆ° aps_work_order_schedule è¡¨")
                work_order_schedule_count = 0
                
                # ä½¿ç”¨åˆå¹¶åçš„è®¡åˆ’æ•°æ®
                for merged_plan in merged_plans:
                    # è·å–åˆå¹¶åè®¡åˆ’çš„åŸºæœ¬ä¿¡æ¯
                    work_order_nr = merged_plan.get('work_order_nr', 'UNKNOWN')
                    article_nr = merged_plan.get('article_nr', 'UNKNOWN')
                    final_quantity = merged_plan.get('final_quantity', 0)
                    quantity_total = merged_plan.get('quantity_total', 0)
                    
                    # è·å–æ—¶é—´ä¿¡æ¯
                    planned_start = _parse_datetime(merged_plan.get('planned_start')) if merged_plan.get('planned_start') else datetime.now()
                    planned_end = _parse_datetime(merged_plan.get('planned_end')) if merged_plan.get('planned_end') else datetime.now()
                    
                    # è·å–æœºå°ä¿¡æ¯ï¼Œå¤„ç†å¯èƒ½çš„é€—å·åˆ†éš”æ ¼å¼
                    maker_codes_str = merged_plan.get('maker_code', '')
                    feeder_codes_str = merged_plan.get('feeder_code', '')
                    
                    # åˆ†å‰²é€—å·åˆ†éš”çš„æœºå°ä»£ç 
                    maker_codes = [code.strip() for code in maker_codes_str.split(',') if code.strip()] if maker_codes_str else ['UNKNOWN']
                    feeder_codes = [code.strip() for code in feeder_codes_str.split(',') if code.strip()] if feeder_codes_str else ['UNKNOWN']
                    
                    try:
                        # æ’å…¥å·¥å•è°ƒåº¦è®°å½• - åˆ†åˆ«ä¸ºæ¯ä¸ªå·åŒ…æœºÃ—å–‚ä¸æœºç»„åˆæ’å…¥
                        schedule_insert_sql = text("""
                        INSERT INTO aps_work_order_schedule (
                            work_order_nr, article_nr, final_quantity, quantity_total,
                            maker_code, feeder_code, planned_start, planned_end,
                            task_id, schedule_status, is_backup, created_time
                        ) VALUES (
                            :work_order_nr, :article_nr, :final_quantity, :quantity_total,
                            :maker_code, :feeder_code, :planned_start, :planned_end,
                            :task_id, :schedule_status, :is_backup, NOW()
                        )
                        """)
                        
                        # ä¸ºæ¯ä¸ªå·åŒ…æœºÃ—å–‚ä¸æœºç»„åˆæ’å…¥ä¸€æ¡è®°å½•ï¼ˆç¬›å¡å°”ç§¯ï¼‰
                        combo_count = 0
                        for maker_code in maker_codes:
                            for feeder_code in feeder_codes:
                                await db.execute(schedule_insert_sql, {
                                    'work_order_nr': work_order_nr,
                                    'article_nr': article_nr,
                                    'final_quantity': final_quantity,
                                    'quantity_total': quantity_total,
                                    'maker_code': maker_code,
                                    'feeder_code': feeder_code,
                                    'planned_start': planned_start,
                                    'planned_end': planned_end,
                                    'task_id': task_id,
                                    'schedule_status': 'COMPLETED',
                                    'is_backup': False
                                })
                                combo_count += 1
                        work_order_schedule_count += combo_count
                        print(f"ğŸ” DEBUG: åˆå¹¶è®¡åˆ’è°ƒåº¦è®°å½•æ’å…¥æˆåŠŸ: {work_order_nr} å…± {combo_count} æ¡ç»„åˆè®°å½•")
                    except Exception as e:
                        print(f"ğŸ” DEBUG: å·¥å•è°ƒåº¦è®°å½•æ’å…¥å¤±è´¥ {work_order_nr}: {e}")
                        # ä¸ä¸­æ–­æµç¨‹ï¼Œç»§ç»­å¤„ç†å…¶ä»–å·¥å•
                
                print(f"ğŸ” DEBUG: å·¥å•è°ƒåº¦æ•°æ®å†™å…¥å®Œæˆï¼Œå…± {work_order_schedule_count} æ¡è®°å½•")
                
                await db.commit()
                
                print(f"ğŸ” DEBUG: äº‹åŠ¡æäº¤æˆåŠŸ!")
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²å®Œæˆ
                task.task_status = SchedulingTaskStatus.COMPLETED
                task.end_time = datetime.now()
                task.current_stage = "å®Œæˆ"
                task.progress = 100
                task.total_records = pipeline_result.get('summary', {}).get('input_records', 0)
                task.processed_records = len(final_work_orders)
                
                if task.start_time:
                    execution_duration = (task.end_time - task.start_time).total_seconds()
                    task.execution_duration = execution_duration
                
                task.result_summary = {
                    "packing_orders_generated": packing_orders_count,
                    "feeding_orders_generated": feeding_orders_count,
                    "work_order_schedules_generated": work_order_schedule_count,
                    "total_work_orders": len(final_work_orders),
                    "execution_summary": pipeline_result.get('summary', {}),
                    "pipeline_stages": len(pipeline_result.get('stages', {}))
                }
                
                await db.commit()
                
            else:
                # ç®¡é“æ‰§è¡Œå¤±è´¥
                task.task_status = SchedulingTaskStatus.FAILED
                task.end_time = datetime.now()
                task.current_stage = "å¤±è´¥"
                task.progress = 0
                task.error_message = pipeline_result.get('error', 'æ’äº§ç®—æ³•æ‰§è¡Œå¤±è´¥')
                
                if task.start_time:
                    execution_duration = (task.end_time - task.start_time).total_seconds()
                    task.execution_duration = execution_duration
                
                await db.commit()
                
        except Exception as e:
            # å¤„ç†å¼‚å¸¸æƒ…å†µ
            try:
                task.task_status = SchedulingTaskStatus.FAILED
                task.end_time = datetime.now()
                task.current_stage = "å¼‚å¸¸"
                task.error_message = f"æ’äº§ç®—æ³•æ‰§è¡Œå¼‚å¸¸ï¼š{str(e)}"
                
                if task.start_time:
                    execution_duration = (task.end_time - task.start_time).total_seconds()
                    task.execution_duration = execution_duration
                
                await db.commit()
            except:
                pass


# ä»»åŠ¡ç®¡ç†ç›¸å…³API
@router.get("/tasks")
async def get_scheduling_tasks(
    status: Optional[str] = None,
    import_batch_id: Optional[str] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    page: int = 1,
    page_size: int = 20,
    db: AsyncSession = Depends(get_async_session)
):
    """
    æŸ¥è¯¢æ’äº§ä»»åŠ¡å†å²è®°å½•
    æ”¯æŒå¤šç»´åº¦ç­›é€‰ï¼šçŠ¶æ€ã€æ‰¹æ¬¡ã€æ—¶é—´èŒƒå›´
    """
    try:
        from sqlalchemy import select, and_, desc, func
        from datetime import datetime
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = []
        if status:
            conditions.append(SchedulingTask.task_status == status)
        if import_batch_id:
            conditions.append(SchedulingTask.import_batch_id == import_batch_id)
        if start_date:
            start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
            conditions.append(SchedulingTask.created_time >= start_dt)
        if end_date:
            end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
            conditions.append(SchedulingTask.created_time <= end_dt)
        
        # æ„å»ºåŸºç¡€æŸ¥è¯¢
        base_query = select(SchedulingTask)
        if conditions:
            base_query = base_query.where(and_(*conditions))
        
        # è·å–æ€»æ•°
        count_query = select(func.count()).select_from(SchedulingTask)
        if conditions:
            count_query = count_query.where(and_(*conditions))
        
        count_result = await db.execute(count_query)
        total_count = count_result.scalar()
        
        # åˆ†é¡µæŸ¥è¯¢
        offset = (page - 1) * page_size
        query = base_query.order_by(desc(SchedulingTask.created_time)).offset(offset).limit(page_size)
        
        result = await db.execute(query)
        tasks = result.scalars().all()
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        task_records = []
        for task in tasks:
            task_records.append({
                "task_id": task.task_id,
                "import_batch_id": task.import_batch_id,
                "task_name": task.task_name,
                "status": task.task_status.value,
                "current_stage": task.current_stage,
                "progress": task.progress,
                "total_records": task.total_records,
                "processed_records": task.processed_records,
                "start_time": task.start_time.isoformat() if task.start_time else None,
                "end_time": task.end_time.isoformat() if task.end_time else None,
                "execution_duration": task.execution_duration,
                "error_message": task.error_message,
                "result_summary": task.result_summary,
                "algorithm_config": {
                    "merge_enabled": task.merge_enabled,
                    "split_enabled": task.split_enabled,
                    "correction_enabled": task.correction_enabled,
                    "parallel_enabled": task.parallel_enabled
                },
                "created_time": task.created_time.isoformat()
            })
        
        # è®¡ç®—åˆ†é¡µä¿¡æ¯
        total_pages = (total_count + page_size - 1) // page_size
        
        return SuccessResponse(
            code=200,
            message="æŸ¥è¯¢æˆåŠŸ",
            data={
                "tasks": task_records,
                "pagination": {
                    "page": page,
                    "page_size": page_size,
                    "total_count": total_count,
                    "total_pages": total_pages,
                    "has_next": page < total_pages,
                    "has_prev": page > 1
                }
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢æ’äº§ä»»åŠ¡å¤±è´¥ï¼š{str(e)}")


@router.get("/tasks/{task_id}")
async def get_scheduling_task_detail(
    task_id: str,
    db: AsyncSession = Depends(get_async_session)
):
    """
    è·å–æ’äº§ä»»åŠ¡å®Œæ•´è¯¦æƒ…
    åŒ…å«ï¼šåŸºæœ¬ä¿¡æ¯ã€æ‰§è¡Œæ—¥å¿—ã€å·¥å•ç»Ÿè®¡ã€å…³è”æ•°æ®
    """
    try:
        from sqlalchemy import select
        from app.models.scheduling_models import ProcessingLog
        
        # æŸ¥è¯¢ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
        task_result = await db.execute(
            select(SchedulingTask).where(SchedulingTask.task_id == task_id)
        )
        task = task_result.scalar_one_or_none()
        
        if not task:
            raise HTTPException(status_code=404, detail=f"æ’äº§ä»»åŠ¡ä¸å­˜åœ¨ï¼š{task_id}")
        
        # æŸ¥è¯¢æ‰§è¡Œæ—¥å¿—
        logs_result = await db.execute(
            select(ProcessingLog).where(ProcessingLog.task_id == task_id)
            .order_by(ProcessingLog.execution_time.desc())
        )
        logs = logs_result.scalars().all()
        
        # è½¬æ¢æ—¥å¿—æ ¼å¼
        log_records = []
        for log in logs:
            log_records.append({
                "id": log.id,
                "stage": log.stage,
                "step_name": log.step_name,
                "level": log.log_level.value,
                "message": log.log_message,
                "execution_time": log.execution_time.isoformat(),
                "duration_ms": log.duration_ms,
                "processing_data": log.processing_data
            })
        
        return SuccessResponse(
            code=200,
            message="æŸ¥è¯¢æˆåŠŸ",
            data={
                "task": {
                    "task_id": task.task_id,
                    "import_batch_id": task.import_batch_id,
                    "task_name": task.task_name,
                    "status": task.task_status.value,
                    "current_stage": task.current_stage,
                    "progress": task.progress,
                    "total_records": task.total_records,
                    "processed_records": task.processed_records,
                    "start_time": task.start_time.isoformat() if task.start_time else None,
                    "end_time": task.end_time.isoformat() if task.end_time else None,
                    "execution_duration": task.execution_duration,
                    "error_message": task.error_message,
                    "result_summary": task.result_summary,
                    "algorithm_config": {
                        "merge_enabled": task.merge_enabled,
                        "split_enabled": task.split_enabled,
                        "correction_enabled": task.correction_enabled,
                        "parallel_enabled": task.parallel_enabled
                    },
                    "created_by": task.created_by,
                    "created_time": task.created_time.isoformat(),
                    "updated_time": task.updated_time.isoformat()
                },
                "logs": log_records,
                "logs_count": len(log_records)
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢ä»»åŠ¡è¯¦æƒ…å¤±è´¥ï¼š{str(e)}")


@router.post("/tasks/{task_id}/retry")
async def retry_scheduling_task(
    task_id: str,
    db: AsyncSession = Depends(get_async_session)
):
    """
    é‡æ–°æ‰§è¡Œå¤±è´¥çš„æ’äº§ä»»åŠ¡
    """
    try:
        from sqlalchemy import select, update
        
        # æŸ¥è¯¢ä»»åŠ¡
        result = await db.execute(
            select(SchedulingTask).where(SchedulingTask.task_id == task_id)
        )
        task = result.scalar_one_or_none()
        
        if not task:
            raise HTTPException(status_code=404, detail=f"æ’äº§ä»»åŠ¡ä¸å­˜åœ¨ï¼š{task_id}")
        
        if task.task_status not in [SchedulingTaskStatus.FAILED, SchedulingTaskStatus.CANCELLED]:
            raise HTTPException(status_code=400, detail="åªèƒ½é‡è¯•å¤±è´¥æˆ–å·²å–æ¶ˆçš„ä»»åŠ¡")
        
        # é‡ç½®ä»»åŠ¡çŠ¶æ€
        await db.execute(
            update(SchedulingTask)
            .where(SchedulingTask.task_id == task_id)
            .values(
                task_status=SchedulingTaskStatus.PENDING,
                current_stage="ç­‰å¾…é‡è¯•",
                progress=0,
                start_time=None,
                end_time=None,
                execution_duration=None,
                error_message=None,
                processed_records=0
            )
        )
        await db.commit()
        
        # TODO: è¿™é‡Œåº”è¯¥è§¦å‘åå°ä»»åŠ¡é‡æ–°æ‰§è¡Œæ’äº§ç®—æ³•
        # ç°åœ¨å…ˆè¿”å›æˆåŠŸçŠ¶æ€ï¼Œåç»­éœ€è¦é›†æˆåå°ä»»åŠ¡ç³»ç»Ÿ
        
        return SuccessResponse(
            code=200,
            message="ä»»åŠ¡é‡è¯•å·²å¯åŠ¨",
            data={
                "task_id": task_id,
                "status": "PENDING",
                "message": "ä»»åŠ¡å·²é‡ç½®ï¼Œæ­£åœ¨å‡†å¤‡é‡æ–°æ‰§è¡Œ"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä»»åŠ¡é‡è¯•å¤±è´¥ï¼š{str(e)}")


@router.post("/tasks/{task_id}/cancel")
async def cancel_scheduling_task(
    task_id: str,
    db: AsyncSession = Depends(get_async_session)
):
    """
    å–æ¶ˆè¿è¡Œä¸­çš„æ’äº§ä»»åŠ¡
    """
    try:
        from sqlalchemy import select, update
        from datetime import datetime
        
        # æŸ¥è¯¢ä»»åŠ¡
        result = await db.execute(
            select(SchedulingTask).where(SchedulingTask.task_id == task_id)
        )
        task = result.scalar_one_or_none()
        
        if not task:
            raise HTTPException(status_code=404, detail=f"æ’äº§ä»»åŠ¡ä¸å­˜åœ¨ï¼š{task_id}")
        
        if task.task_status not in [SchedulingTaskStatus.PENDING, SchedulingTaskStatus.RUNNING]:
            raise HTTPException(status_code=400, detail="åªèƒ½å–æ¶ˆç­‰å¾…ä¸­æˆ–è¿è¡Œä¸­çš„ä»»åŠ¡")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²å–æ¶ˆ
        current_time = datetime.now()
        execution_duration = None
        if task.start_time:
            execution_duration = int((current_time - task.start_time).total_seconds())
        
        await db.execute(
            update(SchedulingTask)
            .where(SchedulingTask.task_id == task_id)
            .values(
                task_status=SchedulingTaskStatus.CANCELLED,
                current_stage="å·²å–æ¶ˆ",
                end_time=current_time,
                execution_duration=execution_duration,
                error_message="ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ"
            )
        )
        await db.commit()
        
        return SuccessResponse(
            code=200,
            message="ä»»åŠ¡å·²å–æ¶ˆ",
            data={
                "task_id": task_id,
                "status": "CANCELLED",
                "message": "ä»»åŠ¡å·²æˆåŠŸå–æ¶ˆ"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä»»åŠ¡å–æ¶ˆå¤±è´¥ï¼š{str(e)}")


@router.get("/tasks/statistics")
async def get_scheduling_statistics(
    days: int = 30,
    db: AsyncSession = Depends(get_async_session)
):
    """
    è·å–æ’äº§ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯
    æˆåŠŸç‡ã€æ‰§è¡Œæ—¶é•¿åˆ†æã€å·¥å•ç”Ÿæˆç»Ÿè®¡
    """
    try:
        from sqlalchemy import select, func, and_
        from datetime import datetime, timedelta
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        # æ€»ä»»åŠ¡æ•°
        total_result = await db.execute(
            select(func.count()).select_from(SchedulingTask)
            .where(SchedulingTask.created_time >= start_date)
        )
        total_tasks = total_result.scalar() or 0
        
        # æˆåŠŸä»»åŠ¡æ•°
        success_result = await db.execute(
            select(func.count()).select_from(SchedulingTask)
            .where(and_(
                SchedulingTask.created_time >= start_date,
                SchedulingTask.task_status == SchedulingTaskStatus.COMPLETED
            ))
        )
        success_tasks = success_result.scalar() or 0
        
        # å¤±è´¥ä»»åŠ¡æ•°
        failed_result = await db.execute(
            select(func.count()).select_from(SchedulingTask)
            .where(and_(
                SchedulingTask.created_time >= start_date,
                SchedulingTask.task_status == SchedulingTaskStatus.FAILED
            ))
        )
        failed_tasks = failed_result.scalar() or 0
        
        # è¿è¡Œä¸­ä»»åŠ¡æ•°
        running_result = await db.execute(
            select(func.count()).select_from(SchedulingTask)
            .where(SchedulingTask.task_status == SchedulingTaskStatus.RUNNING)
        )
        running_tasks = running_result.scalar() or 0
        
        # å¹³å‡æ‰§è¡Œæ—¶é•¿
        avg_duration_result = await db.execute(
            select(func.avg(SchedulingTask.execution_duration)).select_from(SchedulingTask)
            .where(and_(
                SchedulingTask.created_time >= start_date,
                SchedulingTask.task_status == SchedulingTaskStatus.COMPLETED,
                SchedulingTask.execution_duration != None
            ))
        )
        avg_duration = avg_duration_result.scalar() or 0
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = (success_tasks / total_tasks * 100) if total_tasks > 0 else 0
        
        # æ€»å·¥å•ç”Ÿæˆç»Ÿè®¡ï¼ˆåŸºäºresult_summaryï¼‰
        total_work_orders = 0
        packing_orders = 0
        feeding_orders = 0
        
        summary_result = await db.execute(
            select(SchedulingTask.result_summary).select_from(SchedulingTask)
            .where(and_(
                SchedulingTask.created_time >= start_date,
                SchedulingTask.task_status == SchedulingTaskStatus.COMPLETED,
                SchedulingTask.result_summary != None
            ))
        )
        
        for (summary,) in summary_result.fetchall():
            if summary:
                total_work_orders += summary.get('total_work_orders', 0)
                packing_orders += summary.get('packing_orders_generated', 0)
                feeding_orders += summary.get('feeding_orders_generated', 0)
        
        return SuccessResponse(
            code=200,
            message="ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ",
            data={
                "period_days": days,
                "total_tasks": total_tasks,
                "success_tasks": success_tasks,
                "failed_tasks": failed_tasks,
                "running_tasks": running_tasks,
                "success_rate": round(success_rate, 2),
                "avg_duration": round(avg_duration, 2) if avg_duration else 0,
                "work_orders_generated": {
                    "total": total_work_orders,
                    "packing": packing_orders,
                    "feeding": feeding_orders
                }
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ï¼š{str(e)}")


@router.get("/tasks/{task_id}/status")
async def get_scheduling_task_status(
    task_id: str,
    db: AsyncSession = Depends(get_async_session)
):
    """
    æŸ¥è¯¢æ’äº§ä»»åŠ¡çŠ¶æ€æ¥å£
    
    Args:
        task_id: ä»»åŠ¡ID
        
    Returns:
        ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
    """
    try:
        from sqlalchemy import select
        result = await db.execute(
            select(SchedulingTask).where(SchedulingTask.task_id == task_id)
        )
        task = result.scalar_one_or_none()
        
        if not task:
            raise HTTPException(status_code=404, detail=f"æ’äº§ä»»åŠ¡ä¸å­˜åœ¨ï¼š{task_id}")
        
        return SuccessResponse(
            code=200,
            message="æŸ¥è¯¢æˆåŠŸ",
            data={
                "task_id": task.task_id,
                "import_batch_id": task.import_batch_id,
                "task_name": task.task_name,
                "status": task.task_status.value,
                "current_stage": task.current_stage,
                "progress": task.progress,
                "total_records": task.total_records,
                "processed_records": task.processed_records,
                "start_time": task.start_time.isoformat() if task.start_time else None,
                "end_time": task.end_time.isoformat() if task.end_time else None,
                "execution_duration": task.execution_duration,
                "error_message": task.error_message,
                "result_summary": task.result_summary,
                "algorithm_config": {
                    "merge_enabled": task.merge_enabled,
                    "split_enabled": task.split_enabled,
                    "correction_enabled": task.correction_enabled,
                    "parallel_enabled": task.parallel_enabled
                }
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"çŠ¶æ€æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}")


# å·¥å•æŸ¥è¯¢è·¯ç”±
work_orders_router = APIRouter(prefix="/work-orders", tags=["å·¥å•ç®¡ç†"])


@router.get("/work-orders")
async def get_work_orders(
    task_id: Optional[str] = None,
    import_batch_id: Optional[str] = None,
    order_type: Optional[str] = None,
    status: Optional[str] = None,
    page: int = 1,
    page_size: int = 20,
    db: AsyncSession = Depends(get_async_session)
):
    """
    æŸ¥è¯¢å·¥å•åˆ—è¡¨æ¥å£
    
    Args:
        task_id: æ’äº§ä»»åŠ¡IDè¿‡æ»¤
        import_batch_id: å¯¼å…¥æ‰¹æ¬¡IDè¿‡æ»¤
        order_type: å·¥å•ç±»å‹è¿‡æ»¤ (HJB-å·åŒ…æœº, HWS-å–‚ä¸æœº)
        status: å·¥å•çŠ¶æ€è¿‡æ»¤
        page: é¡µç 
        page_size: æ¯é¡µå¤§å°
        
    Returns:
        å·¥å•åˆ—è¡¨
    """
    try:
        from sqlalchemy import select, func, and_, outerjoin, or_
        
        # æŸ¥è¯¢çœŸå®å·¥å•æ•°æ®
        work_orders = []
        total_count = 0
        
        # æŸ¥è¯¢å·åŒ…æœºå·¥å•
        if not order_type or order_type == 'HJB':
            packing_query = select(PackingOrder)
            
            # æ·»åŠ è¿‡æ»¤æ¡ä»¶
            conditions = []
            if task_id:
                conditions.append(PackingOrder.task_id == task_id)
            if import_batch_id:
                # é€šè¿‡ import_batch_id æŸ¥æ‰¾å¯¹åº”çš„ä»»åŠ¡ï¼Œç„¶åé€šè¿‡ task_id åŒ¹é…
                task_result = await db.execute(
                    select(SchedulingTask.task_id).where(SchedulingTask.import_batch_id == import_batch_id)
                )
                matching_task_ids = [row[0] for row in task_result.fetchall()]
                if matching_task_ids:
                    conditions.append(PackingOrder.task_id.in_(matching_task_ids))
            if status:
                conditions.append(PackingOrder.order_status == status)
                    
            if conditions:
                packing_query = packing_query.where(and_(*conditions))
            
            packing_result = await db.execute(packing_query)
            packing_orders = packing_result.scalars().all()
            
            for order in packing_orders:
                work_orders.append({
                    "work_order_no": order.plan_id,
                    "order_type": "PACKING",
                    "machine_type": "å·åŒ…æœº", 
                    "production_line": order.production_line,
                    "material_code": order.material_code,
                    "total_quantity": order.quantity,
                    "order_status": order.order_status,
                    "planned_start_time": order.plan_start_time.isoformat() if order.plan_start_time else None,
                    "planned_finish_time": order.plan_end_time.isoformat() if order.plan_end_time else None,
                    "task_id": order.task_id,
                    "created_time": order.created_time.isoformat() if order.created_time else None,
                    "updated_time": order.updated_time.isoformat() if order.updated_time else None
                })
        
        # æŸ¥è¯¢å–‚ä¸æœºå·¥å•
        if not order_type or order_type == 'HWS':
            feeding_query = select(FeedingOrder)
            
            # æ·»åŠ è¿‡æ»¤æ¡ä»¶
            conditions = []
            if task_id:
                conditions.append(FeedingOrder.task_id == task_id)
            if import_batch_id:
                # é€šè¿‡ import_batch_id æŸ¥æ‰¾å¯¹åº”çš„ä»»åŠ¡ï¼Œç„¶åé€šè¿‡ task_id åŒ¹é…
                task_result = await db.execute(
                    select(SchedulingTask.task_id).where(SchedulingTask.import_batch_id == import_batch_id)
                )
                matching_task_ids = [row[0] for row in task_result.fetchall()]
                if matching_task_ids:
                    conditions.append(FeedingOrder.task_id.in_(matching_task_ids))
            if status:
                conditions.append(FeedingOrder.order_status == status)
                    
            if conditions:
                feeding_query = feeding_query.where(and_(*conditions))
            
            feeding_result = await db.execute(feeding_query)
            feeding_orders = feeding_result.scalars().all()
            
            for order in feeding_orders:
                work_orders.append({
                    "work_order_no": order.plan_id,
                    "order_type": "FEEDING",
                    "machine_type": "å–‚ä¸æœº",
                    "equipment_code": order.production_line,
                    "material_code": order.material_code,
                    "total_quantity": order.quantity if order.quantity else 0,
                    "order_status": order.order_status,
                    "planned_start_time": order.plan_start_time.isoformat() if order.plan_start_time else None,
                    "planned_finish_time": order.plan_end_time.isoformat() if order.plan_end_time else None,
                    "task_id": order.task_id,
                    "created_time": order.created_time.isoformat() if order.created_time else None,
                    "updated_time": order.updated_time.isoformat() if order.updated_time else None
                })
        
        total_count = len(work_orders)
        
        # ç®€å•åˆ†é¡µå¤„ç†
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        paginated_orders = work_orders[start_idx:end_idx]
        
        return SuccessResponse(
            code=200,
            message="æŸ¥è¯¢æˆåŠŸ",
            data={
                "work_orders": paginated_orders,
                "total_count": total_count,
                "page": page,
                "page_size": page_size,
                "total_pages": (total_count + page_size - 1) // page_size
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å·¥å•æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}")