"""
å®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯• - éªŒè¯ä»æ—¬è®¡åˆ’æ•°æ®åˆ°å·¥å•ç”Ÿæˆçš„å®Œæ•´æµç¨‹
"""
import pytest
from datetime import datetime
from app.algorithms.pipeline import AlgorithmPipeline
from app.services.database_query_service import DatabaseQueryService


class TestEndToEndFlow:
    """ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_complete_data_flow(self):
        """æµ‹è¯•å®Œæ•´çš„æ•°æ®æµï¼šæ—¬è®¡åˆ’ â†’ ç®—æ³•å¤„ç† â†’ å·¥å•ç”Ÿæˆ"""
        
        # 1. éªŒè¯èƒ½æŸ¥è¯¢æ—¬è®¡åˆ’æ•°æ®
        decade_plans = await DatabaseQueryService.get_decade_plans()
        print(f"âœ“ æŸ¥è¯¢åˆ° {len(decade_plans)} æ¡æ—¬è®¡åˆ’æ•°æ®")
        
        # 2. éªŒè¯èƒ½æŸ¥è¯¢è½®ä¿è®¡åˆ’
        maintenance_plans = await DatabaseQueryService.get_maintenance_plans()
        print(f"âœ“ æŸ¥è¯¢åˆ° {len(maintenance_plans)} æ¡è½®ä¿è®¡åˆ’")
        
        # 3. éªŒè¯èƒ½æŸ¥è¯¢ç­æ¬¡é…ç½®
        shift_configs = await DatabaseQueryService.get_shift_config()
        print(f"âœ“ æŸ¥è¯¢åˆ° {len(shift_configs)} ä¸ªç­æ¬¡é…ç½®")
        
        # 4. éªŒè¯èƒ½æŸ¥è¯¢æœºå°å…³ç³»
        machine_relations = await DatabaseQueryService.get_machine_relations()
        print(f"âœ“ æŸ¥è¯¢åˆ° {len(machine_relations)} ä¸ªæœºå°å…³ç³»")
        
        # 5. åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰çœŸå®æ•°æ®ï¼‰
        if not decade_plans:
            test_data = self._create_test_decade_plan_data()
            print("âœ“ ä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œç®—æ³•éªŒè¯")
        else:
            test_data = decade_plans[:5]  # ä½¿ç”¨å‰5æ¡çœŸå®æ•°æ®
            print(f"âœ“ ä½¿ç”¨ {len(test_data)} æ¡çœŸå®æ•°æ®è¿›è¡Œç®—æ³•éªŒè¯")
        
        # 6. æµ‹è¯•å®Œæ•´ç®—æ³•ç®¡é“
        pipeline = AlgorithmPipeline()
        
        # æµ‹è¯•æ•°æ®é¢„å¤„ç†
        preprocessing_result = await pipeline.preprocessor.process_with_real_data(test_data)
        assert preprocessing_result.metrics.custom_metrics.get('used_real_database_data') == True
        print(f"âœ“ æ•°æ®é¢„å¤„ç†: {len(preprocessing_result.output_data)} æ¡æœ‰æ•ˆæ•°æ®")
        
        # æµ‹è¯•è§„åˆ™åˆå¹¶
        merge_result = await pipeline.merger.process_with_real_data(preprocessing_result.output_data)
        assert merge_result.metrics.custom_metrics.get('used_real_database_data') == True
        print(f"âœ“ è§„åˆ™åˆå¹¶: {len(merge_result.output_data)} æ¡åˆå¹¶åæ•°æ®")
        
        # æµ‹è¯•è§„åˆ™æ‹†åˆ†
        split_result = await pipeline.splitter.process_with_real_data(merge_result.output_data)
        assert split_result.metrics.custom_metrics.get('used_real_database_data') == True
        print(f"âœ“ è§„åˆ™æ‹†åˆ†: {len(split_result.output_data)} æ¡æ‹†åˆ†åæ•°æ®")
        
        # æµ‹è¯•æ—¶é—´æ ¡æ­£
        time_correction_result = await pipeline.time_corrector.process_with_real_data(split_result.output_data)
        assert time_correction_result.metrics.custom_metrics.get('used_real_database_data') == True
        print(f"âœ“ æ—¶é—´æ ¡æ­£: {len(time_correction_result.output_data)} æ¡æ ¡æ­£åæ•°æ®")
        
        # æµ‹è¯•å¹¶è¡Œå¤„ç†
        parallel_result = await pipeline.parallel_processor.process_with_real_data(time_correction_result.output_data)
        assert parallel_result.metrics.custom_metrics.get('used_real_database_data') == True
        print(f"âœ“ å¹¶è¡Œå¤„ç†: {len(parallel_result.output_data)} æ¡å¹¶è¡Œæ•°æ®")
        
        # æµ‹è¯•å·¥å•ç”Ÿæˆ
        work_order_result = await pipeline.work_order_generator.process_with_real_data(parallel_result.output_data)
        assert work_order_result.metrics.custom_metrics.get('used_real_database_data') == True
        print(f"âœ“ å·¥å•ç”Ÿæˆ: {len(work_order_result.output_data)} ä¸ªå·¥å•")
        
        # 7. éªŒè¯æœ€ç»ˆå·¥å•çš„å®Œæ•´æ€§
        final_work_orders = work_order_result.output_data
        if final_work_orders:
            sample_order = final_work_orders[0]
            required_fields = [
                'work_order_nr', 'work_order_type', 'machine_type',
                'machine_code', 'product_code', 'plan_quantity',
                'planned_start_time', 'planned_end_time', 'work_order_status'
            ]
            
            for field in required_fields:
                assert field in sample_order, f"å·¥å•ç¼ºå°‘å¿…è¦å­—æ®µ: {field}"
            
            print(f"âœ“ å·¥å•å®Œæ•´æ€§éªŒè¯é€šè¿‡ - æ ·ä¾‹å·¥å•å·: {sample_order.get('work_order_nr')}")
        
        print("ğŸ‰ ç«¯åˆ°ç«¯æµ‹è¯•å®Œå…¨é€šè¿‡ï¼")
        
        return {
            'decade_plans_count': len(decade_plans),
            'maintenance_plans_count': len(maintenance_plans), 
            'shift_configs_count': len(shift_configs),
            'machine_relations_count': len(machine_relations),
            'final_work_orders_count': len(final_work_orders),
            'test_passed': True
        }
    
    def _create_test_decade_plan_data(self):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„æ—¬è®¡åˆ’æ•°æ®"""
        return [
            {
                'id': 1,
                'import_batch_id': 'test_batch_001',
                'work_order_nr': 'WO20241201001',
                'article_nr': 'çº¢å¡”å±±(ç¡¬)',
                'package_type': 'ç¡¬åŒ…',
                'specification': 'é•¿å˜´',
                'quantity_total': 1000,
                'final_quantity': 950,
                'production_unit': 'ç”Ÿäº§ä¸€éƒ¨',
                'maker_code': 'JJ01',
                'feeder_code': 'WS01',
                'planned_start': datetime(2024, 12, 1, 8, 0),
                'planned_end': datetime(2024, 12, 1, 16, 0),
                'production_date_range': '2024-12-01',
                'validation_status': 'VALID',
                'validation_message': None
            },
            {
                'id': 2,
                'import_batch_id': 'test_batch_001',
                'work_order_nr': 'WO20241201002',
                'article_nr': 'äº‘çƒŸ(è½¯)',
                'package_type': 'è½¯åŒ…',
                'specification': 'çŸ­å˜´',
                'quantity_total': 800,
                'final_quantity': 760,
                'production_unit': 'ç”Ÿäº§äºŒéƒ¨',
                'maker_code': 'JJ02',
                'feeder_code': 'WS02',
                'planned_start': datetime(2024, 12, 1, 9, 0),
                'planned_end': datetime(2024, 12, 1, 17, 0),
                'production_date_range': '2024-12-01',
                'validation_status': 'VALID',
                'validation_message': None
            }
        ]
    
    @pytest.mark.asyncio
    async def test_database_connectivity(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥æ€§"""
        try:
            # æµ‹è¯•æ¯ä¸ªæ•°æ®åº“æŸ¥è¯¢æ–¹æ³•
            decade_plans = await DatabaseQueryService.get_decade_plans()
            maintenance_plans = await DatabaseQueryService.get_maintenance_plans()
            shift_configs = await DatabaseQueryService.get_shift_config()
            machine_relations = await DatabaseQueryService.get_machine_relations()
            
            print(f"æ•°æ®åº“è¿æ¥æµ‹è¯•é€šè¿‡:")
            print(f"  - aps_decade_plan: {len(decade_plans)} æ¡è®°å½•")
            print(f"  - aps_maintenance_plan: {len(maintenance_plans)} æ¡è®°å½•")
            print(f"  - aps_shift_config: {len(shift_configs)} æ¡è®°å½•")
            print(f"  - aps_machine_relation: {len(machine_relations)} æ¡è®°å½•")
            
            assert True  # å¦‚æœæ²¡æœ‰å¼‚å¸¸å°±é€šè¿‡
            
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            pytest.fail(f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
    
    @pytest.mark.asyncio
    async def test_algorithm_with_empty_data(self):
        """æµ‹è¯•ç®—æ³•å¤„ç†ç©ºæ•°æ®çš„å¥å£®æ€§"""
        pipeline = AlgorithmPipeline()
        
        # æµ‹è¯•ç©ºæ•°æ®å¤„ç†
        empty_result = await pipeline.execute_full_pipeline([], use_real_data=True)
        
        assert empty_result['success'] == True
        assert empty_result['summary']['input_records'] == 0
        assert empty_result['summary']['output_work_orders'] == 0
        
        print("âœ“ ç©ºæ•°æ®å¤„ç†æµ‹è¯•é€šè¿‡")