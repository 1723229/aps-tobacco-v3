"""
T014 æµ‹è¯•åŠ©æ‰‹å·¥å…·

æä¾›Excelè§£æé›†æˆæµ‹è¯•çš„è¾…åŠ©åŠŸèƒ½ï¼š
- æµ‹è¯•æ•°æ®éªŒè¯
- æ€§èƒ½ç›‘æ§
- å¹¶å‘æµ‹è¯•ç®¡ç†
- é”™è¯¯æ¨¡æ‹Ÿ
"""

import time
import threading
import psutil
import gc
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import os


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    execution_time: float
    memory_usage_mb: float
    memory_peak_mb: float
    cpu_usage_percent: float
    records_processed: int
    records_per_second: float
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'execution_time': self.execution_time,
            'memory_usage_mb': self.memory_usage_mb,
            'memory_peak_mb': self.memory_peak_mb,
            'cpu_usage_percent': self.cpu_usage_percent,
            'records_processed': self.records_processed,
            'records_per_second': self.records_per_second
        }


@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœæ•°æ®ç±»"""
    is_valid: bool
    error_count: int
    warning_count: int
    missing_fields: List[str]
    invalid_values: List[str]
    format_errors: List[str]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'is_valid': self.is_valid,
            'error_count': self.error_count,
            'warning_count': self.warning_count,
            'missing_fields': self.missing_fields,
            'invalid_values': self.invalid_values,
            'format_errors': self.format_errors
        }


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.start_time: Optional[float] = None
        self.start_memory: Optional[float] = None
        self.peak_memory: float = 0
        self.cpu_samples: List[float] = []
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.start_time = time.time()
        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        self.peak_memory = self.start_memory
        self.cpu_samples = []
        
        # å¯åŠ¨CPUç›‘æ§çº¿ç¨‹
        self._start_cpu_monitoring()
    
    def _start_cpu_monitoring(self):
        """å¯åŠ¨CPUç›‘æ§çº¿ç¨‹"""
        def monitor_cpu():
            while self.start_time and time.time() - self.start_time < 300:  # æœ€å¤šç›‘æ§5åˆ†é’Ÿ
                try:
                    cpu_percent = self.process.cpu_percent(interval=0.1)
                    self.cpu_samples.append(cpu_percent)
                    
                    current_memory = self.process.memory_info().rss / 1024 / 1024
                    self.peak_memory = max(self.peak_memory, current_memory)
                    
                    time.sleep(0.5)
                except:
                    break
        
        thread = threading.Thread(target=monitor_cpu, daemon=True)
        thread.start()
    
    def stop_monitoring(self, records_processed: int = 0) -> PerformanceMetrics:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»“æœ"""
        if not self.start_time or not self.start_memory:
            raise ValueError("ç›‘æ§æœªå¯åŠ¨")
        
        end_time = time.time()
        execution_time = end_time - self.start_time
        
        end_memory = self.process.memory_info().rss / 1024 / 1024
        memory_usage = end_memory - self.start_memory
        
        avg_cpu = sum(self.cpu_samples) / len(self.cpu_samples) if self.cpu_samples else 0
        
        records_per_second = records_processed / execution_time if execution_time > 0 else 0
        
        # é‡ç½®ç›‘æ§çŠ¶æ€
        self.start_time = None
        self.start_memory = None
        
        return PerformanceMetrics(
            execution_time=execution_time,
            memory_usage_mb=memory_usage,
            memory_peak_mb=self.peak_memory,
            cpu_usage_percent=avg_cpu,
            records_processed=records_processed,
            records_per_second=records_per_second
        )


class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    # æ­å·å‚å¿…éœ€å­—æ®µ
    REQUIRED_FIELDS = [
        'monthly_work_order_nr',
        'monthly_article_nr', 
        'monthly_article_name',
        'monthly_target_quantity',
        'monthly_plan_year',
        'monthly_plan_month'
    ]
    
    # å¯é€‰å­—æ®µ
    OPTIONAL_FIELDS = [
        'monthly_specification',
        'monthly_package_type',
        'monthly_planned_boxes',
        'monthly_feeder_codes',
        'monthly_maker_codes',
        'monthly_planned_start',
        'monthly_planned_end',
        'monthly_extraction_notes'
    ]
    
    @classmethod
    def validate_record(cls, record: Dict[str, Any]) -> ValidationResult:
        """éªŒè¯å•æ¡è®°å½•"""
        missing_fields = []
        invalid_values = []
        format_errors = []
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in cls.REQUIRED_FIELDS:
            if field not in record or record[field] is None:
                missing_fields.append(field)
        
        # éªŒè¯æ•°æ®æ ¼å¼
        if 'monthly_work_order_nr' in record:
            work_order = record['monthly_work_order_nr']
            if work_order and not work_order.startswith('HZWO'):
                format_errors.append(f"å·¥å•å·æ ¼å¼é”™è¯¯: {work_order}")
        
        if 'monthly_article_nr' in record:
            article_nr = record['monthly_article_nr']
            if article_nr and not article_nr.startswith('HNZJHYLC'):
                format_errors.append(f"ç‰Œå·ä»£ç æ ¼å¼é”™è¯¯: {article_nr}")
        
        if 'monthly_target_quantity' in record:
            quantity = record['monthly_target_quantity']
            if quantity is not None:
                try:
                    float_quantity = float(quantity)
                    if float_quantity <= 0:
                        invalid_values.append(f"ç›®æ ‡äº§é‡å¿…é¡»å¤§äº0: {quantity}")
                except (ValueError, TypeError):
                    format_errors.append(f"ç›®æ ‡äº§é‡æ ¼å¼é”™è¯¯: {quantity}")
        
        if 'monthly_plan_year' in record:
            year = record['monthly_plan_year']
            if year is not None:
                try:
                    int_year = int(year)
                    if not (2020 <= int_year <= 2030):
                        invalid_values.append(f"è®¡åˆ’å¹´ä»½è¶…å‡ºèŒƒå›´: {year}")
                except (ValueError, TypeError):
                    format_errors.append(f"è®¡åˆ’å¹´ä»½æ ¼å¼é”™è¯¯: {year}")
        
        if 'monthly_plan_month' in record:
            month = record['monthly_plan_month']
            if month is not None:
                try:
                    int_month = int(month)
                    if not (1 <= int_month <= 12):
                        invalid_values.append(f"è®¡åˆ’æœˆä»½è¶…å‡ºèŒƒå›´: {month}")
                except (ValueError, TypeError):
                    format_errors.append(f"è®¡åˆ’æœˆä»½æ ¼å¼é”™è¯¯: {month}")
        
        # éªŒè¯æœºå°ä»£ç æ ¼å¼
        for field_name, prefix in [('monthly_feeder_codes', 'F'), ('monthly_maker_codes', 'M')]:
            if field_name in record and record[field_name]:
                codes = record[field_name].split(',')
                for code in codes:
                    code = code.strip()
                    if code and not code.startswith(prefix):
                        format_errors.append(f"{field_name}æ ¼å¼é”™è¯¯: {code}")
        
        # è®¡ç®—ç»“æœ
        is_valid = len(missing_fields) == 0 and len(invalid_values) == 0 and len(format_errors) == 0
        error_count = len(missing_fields) + len(invalid_values) + len(format_errors)
        warning_count = 0  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ è­¦å‘Šé€»è¾‘
        
        return ValidationResult(
            is_valid=is_valid,
            error_count=error_count,
            warning_count=warning_count,
            missing_fields=missing_fields,
            invalid_values=invalid_values,
            format_errors=format_errors
        )
    
    @classmethod
    def validate_batch(cls, records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """éªŒè¯è®°å½•æ‰¹æ¬¡"""
        results = []
        total_errors = 0
        total_warnings = 0
        
        for i, record in enumerate(records):
            result = cls.validate_record(record)
            results.append({
                'record_index': i,
                'validation_result': result.to_dict()
            })
            total_errors += result.error_count
            total_warnings += result.warning_count
        
        valid_records = sum(1 for r in results if r['validation_result']['is_valid'])
        
        return {
            'total_records': len(records),
            'valid_records': valid_records,
            'invalid_records': len(records) - valid_records,
            'total_errors': total_errors,
            'total_warnings': total_warnings,
            'validation_details': results
        }


class ConcurrentTestManager:
    """å¹¶å‘æµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
        self.results: Dict[str, Any] = {}
        self.errors: List[str] = []
        self.lock = threading.Lock()
    
    def run_concurrent_tests(
        self, 
        test_functions: List[Callable],
        test_data: List[Any]
    ) -> Dict[str, Any]:
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        if len(test_functions) != len(test_data):
            raise ValueError("æµ‹è¯•å‡½æ•°å’Œæµ‹è¯•æ•°æ®æ•°é‡ä¸åŒ¹é…")
        
        threads = []
        start_time = time.time()
        
        for i, (test_func, data) in enumerate(zip(test_functions, test_data)):
            thread = threading.Thread(
                target=self._run_single_test,
                args=(f"test_{i}", test_func, data)
            )
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        end_time = time.time()
        
        return {
            'total_tests': len(test_functions),
            'successful_tests': len([r for r in self.results.values() if r.get('success', False)]),
            'failed_tests': len(self.errors),
            'execution_time': end_time - start_time,
            'results': self.results,
            'errors': self.errors
        }
    
    def _run_single_test(self, test_id: str, test_func: Callable, data: Any):
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        try:
            result = test_func(data)
            with self.lock:
                self.results[test_id] = {
                    'success': True,
                    'result': result,
                    'thread_id': threading.current_thread().ident
                }
        except Exception as e:
            with self.lock:
                self.errors.append(f"{test_id}: {str(e)}")
                self.results[test_id] = {
                    'success': False,
                    'error': str(e),
                    'thread_id': threading.current_thread().ident
                }


class ErrorSimulator:
    """é”™è¯¯æ¨¡æ‹Ÿå™¨"""
    
    @staticmethod
    def simulate_file_corruption(file_path: str) -> str:
        """æ¨¡æ‹Ÿæ–‡ä»¶æŸå"""
        corrupted_path = file_path.replace('.xlsx', '_corrupted.xlsx')
        
        # åˆ›å»ºæŸåçš„æ–‡ä»¶ï¼ˆåªå¤åˆ¶éƒ¨åˆ†å†…å®¹ï¼‰
        with open(file_path, 'rb') as original:
            content = original.read()
            # åªä¿ç•™å‰50%çš„å†…å®¹
            corrupted_content = content[:len(content)//2]
        
        with open(corrupted_path, 'wb') as corrupted:
            corrupted.write(corrupted_content)
        
        return corrupted_path
    
    @staticmethod
    def simulate_permission_error(file_path: str) -> str:
        """æ¨¡æ‹Ÿæƒé™é”™è¯¯ï¼ˆåœ¨æ”¯æŒçš„ç³»ç»Ÿä¸Šï¼‰"""
        try:
            os.chmod(file_path, 0o000)  # ç§»é™¤æ‰€æœ‰æƒé™
            return file_path
        except:
            # å¦‚æœæ— æ³•ä¿®æ”¹æƒé™ï¼Œè¿”å›åŸæ–‡ä»¶
            return file_path
    
    @staticmethod
    def simulate_memory_pressure():
        """æ¨¡æ‹Ÿå†…å­˜å‹åŠ›"""
        # åˆ†é…å¤§é‡å†…å­˜ä»¥æ¨¡æ‹Ÿå†…å­˜å‹åŠ›
        memory_hog = []
        try:
            for i in range(1000):
                memory_hog.append([0] * 100000)  # åˆ†é…çº¦100MB
                if i % 100 == 0:
                    time.sleep(0.01)  # ç»™å…¶ä»–çº¿ç¨‹æœºä¼š
        except MemoryError:
            pass
        finally:
            # æ¸…ç†å†…å­˜
            del memory_hog
            gc.collect()


class TestReporter:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.test_results: List[Dict[str, Any]] = []
        self.performance_data: List[PerformanceMetrics] = []
    
    def add_test_result(
        self, 
        test_name: str, 
        success: bool, 
        details: Dict[str, Any],
        performance: Optional[PerformanceMetrics] = None
    ):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        result = {
            'test_name': test_name,
            'success': success,
            'timestamp': datetime.now().isoformat(),
            'details': details
        }
        
        if performance:
            result['performance'] = performance.to_dict()
            self.performance_data.append(performance)
        
        self.test_results.append(result)
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()
        
        successful_tests = [r for r in self.test_results if r['success']]
        failed_tests = [r for r in self.test_results if not r['success']]
        
        # æ€§èƒ½ç»Ÿè®¡
        performance_summary = {}
        if self.performance_data:
            performance_summary = {
                'avg_execution_time': sum(p.execution_time for p in self.performance_data) / len(self.performance_data),
                'max_execution_time': max(p.execution_time for p in self.performance_data),
                'avg_memory_usage': sum(p.memory_usage_mb for p in self.performance_data) / len(self.performance_data),
                'max_memory_usage': max(p.memory_usage_mb for p in self.performance_data),
                'total_records_processed': sum(p.records_processed for p in self.performance_data),
                'avg_records_per_second': sum(p.records_per_second for p in self.performance_data) / len(self.performance_data)
            }
        
        return {
            'test_session': {
                'start_time': self.start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'total_duration_seconds': total_duration
            },
            'summary': {
                'total_tests': len(self.test_results),
                'successful_tests': len(successful_tests),
                'failed_tests': len(failed_tests),
                'success_rate': len(successful_tests) / len(self.test_results) if self.test_results else 0
            },
            'performance_summary': performance_summary,
            'test_details': self.test_results,
            'failed_test_details': failed_tests
        }
    
    def save_report(self, file_path: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report = self.generate_report()
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)


# æµ‹è¯•åŠ©æ‰‹åŠŸèƒ½
if __name__ == "__main__":
    print("ğŸ”§ æµ‹è¯•T014åŠ©æ‰‹å·¥å…·...")
    
    # æµ‹è¯•æ€§èƒ½ç›‘æ§
    print("\nğŸ“Š æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨:")
    monitor = PerformanceMonitor()
    monitor.start_monitoring()
    
    # æ¨¡æ‹Ÿä¸€äº›å·¥ä½œ
    time.sleep(1)
    test_data = [i for i in range(10000)]
    
    metrics = monitor.stop_monitoring(records_processed=len(test_data))
    print(f"   æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}ç§’")
    print(f"   å†…å­˜ä½¿ç”¨: {metrics.memory_usage_mb:.1f}MB")
    print(f"   å¤„ç†é€Ÿåº¦: {metrics.records_per_second:.0f}è®°å½•/ç§’")
    
    # æµ‹è¯•æ•°æ®éªŒè¯
    print("\nâœ… æµ‹è¯•æ•°æ®éªŒè¯å™¨:")
    test_record = {
        'monthly_work_order_nr': 'HZWO202412001',
        'monthly_article_nr': 'HNZJHYLC001',
        'monthly_article_name': 'åˆ©ç¾¤ï¼ˆé˜³å…‰ï¼‰',
        'monthly_target_quantity': 120.5,
        'monthly_plan_year': 2024,
        'monthly_plan_month': 12
    }
    
    validation_result = DataValidator.validate_record(test_record)
    print(f"   éªŒè¯ç»“æœ: {'é€šè¿‡' if validation_result.is_valid else 'å¤±è´¥'}")
    print(f"   é”™è¯¯æ•°é‡: {validation_result.error_count}")
    
    # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
    print("\nğŸ“„ æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨:")
    reporter = TestReporter()
    reporter.add_test_result("ç¤ºä¾‹æµ‹è¯•", True, {"è®°å½•æ•°": 100}, metrics)
    
    report = reporter.generate_report()
    print(f"   æµ‹è¯•æ€»æ•°: {report['summary']['total_tests']}")
    print(f"   æˆåŠŸç‡: {report['summary']['success_rate']:.1%}")
    
    print("\nâœ… T014åŠ©æ‰‹å·¥å…·æµ‹è¯•å®Œæˆ")