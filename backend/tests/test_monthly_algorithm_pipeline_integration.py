"""
T015 - APSæœˆåº¦ç®—æ³•ç®¡é“æ‰§è¡Œé›†æˆæµ‹è¯•

æµ‹è¯•ç›®çš„: éªŒè¯æœˆåº¦æ’äº§ç®—æ³•ç®¡é“çš„ç«¯åˆ°ç«¯æ‰§è¡Œæµç¨‹å’Œæ€§èƒ½
æµ‹è¯•ç­–ç•¥: é›†æˆæµ‹è¯• - éªŒè¯ç®—æ³•æ¨¡å—é—´çš„åè°ƒã€æ•°æ®æµè½¬å’Œç»“æœæ­£ç¡®æ€§
TDDè¦æ±‚: æµ‹è¯•å®Œæ•´çš„æœˆåº¦ç®—æ³•ç®¡é“åŠŸèƒ½ï¼Œç¡®ä¿æ‰€æœ‰æ¨¡å—æ­£ç¡®åä½œ

é›†æˆæµ‹è¯•å†…å®¹:
1. å®Œæ•´ç®—æ³•ç®¡é“æ‰§è¡Œ - ä»å·¥ä½œæ—¥å†åˆ°ç»“æœæ ¼å¼åŒ–çš„8ä¸ªæ­¥éª¤
2. ç®—æ³•æ¨¡å—é—´æ•°æ®æµéªŒè¯ - ç¡®ä¿æ•°æ®æ­£ç¡®ä¼ é€’å’Œè½¬æ¢
3. å¹¶è¡Œæ‰§è¡Œå’Œæ€§èƒ½ä¼˜åŒ– - éªŒè¯å¹¶å‘å¤„ç†èƒ½åŠ›
4. çº¦æŸéµå¾ªå’Œç»“æœéªŒè¯ - ç¡®ä¿ä¸šåŠ¡è§„åˆ™æ­£ç¡®åº”ç”¨
5. å¼‚å¸¸å¤„ç†å’Œé”™è¯¯æ¢å¤ - æµ‹è¯•å®¹é”™èƒ½åŠ›
6. å†…å­˜å’Œæ€§èƒ½åŸºå‡†æµ‹è¯• - éªŒè¯æ€§èƒ½æŒ‡æ ‡
7. å·¥ä½œæ—¥å†é›†æˆæµ‹è¯• - éªŒè¯æ—¥å†çº¦æŸ
8. çœŸå®æ•°æ®åœºæ™¯æµ‹è¯• - ä½¿ç”¨å®é™…ä¸šåŠ¡æ•°æ®
"""

import pytest
import asyncio
import time
import gc
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from unittest.mock import AsyncMock, MagicMock, patch
import threading
from concurrent.futures import ThreadPoolExecutor
import psutil
import os

# å¯¼å…¥ç®—æ³•æ¨¡å—
from app.algorithms.monthly_scheduling import (
    MonthlyCalendarService,
    MonthlyMachineSelector,
    MonthlyCapacityCalculator,
    get_algorithm_info,
    get_pipeline_order,
    ALGORITHM_PIPELINE,
    ALGORITHM_MODULES
)

from app.algorithms.monthly_scheduling.base import (
    BaseAlgorithm,
    AlgorithmType,
    Priority,
    MachineType,
    MonthlyPlanItem,
    MachineInfo,
    ScheduleResult,
    ALGORITHM_CONFIG,
    SHIFT_CONFIG,
    calculate_working_hours,
    generate_schedule_id,
    calculate_priority_score,
    MonthlySchedulingError,
    ConstraintViolationError,
    ResourceConflictError,
    AlgorithmTimeoutError
)

# å¯¼å…¥æµ‹è¯•åŠ©æ‰‹
from tests.fixtures.test_helpers import (
    PerformanceMonitor,
    DataValidator,
    ConcurrentTestManager,
    TestReporter,
    PerformanceMetrics,
    ValidationResult
)


# =============================================================================
# æµ‹è¯•æ•°æ®ç±»å’Œé…ç½®
# =============================================================================

@dataclass
class PipelineExecutionResult:
    """ç®¡é“æ‰§è¡Œç»“æœ"""
    success: bool
    execution_time: float
    processed_plans: int
    generated_schedules: int
    memory_usage_mb: float
    algorithm_results: Dict[str, Any]
    errors: List[str]
    warnings: List[str]
    performance_metrics: Dict[str, float]
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class AlgorithmStepResult:
    """ç®—æ³•æ­¥éª¤ç»“æœ"""
    algorithm_name: str
    success: bool
    execution_time: float
    input_size: int
    output_size: int
    memory_delta_mb: float
    cpu_usage_percent: float
    error_message: Optional[str] = None
    warnings: List[str] = None
    
    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []


class MockAlgorithmModule:
    """æ¨¡æ‹Ÿç®—æ³•æ¨¡å— - ç”¨äºæµ‹è¯•ç®¡é“æ‰§è¡Œ"""
    
    def __init__(self, algorithm_type: AlgorithmType, processing_time: float = 0.1):
        self.algorithm_type = algorithm_type
        self.processing_time = processing_time
        self.call_count = 0
        self.last_input = None
        
    async def execute(self, input_data: Any, **kwargs) -> Any:
        """æ¨¡æ‹Ÿç®—æ³•æ‰§è¡Œ"""
        self.call_count += 1
        self.last_input = input_data
        
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        if self.processing_time > 0:
            await asyncio.sleep(self.processing_time)
        
        # æ ¹æ®ç®—æ³•ç±»å‹ç”Ÿæˆç›¸åº”çš„è¾“å‡º
        if self.algorithm_type == AlgorithmType.CALENDAR_SERVICE:
            return self._generate_calendar_output(input_data)
        elif self.algorithm_type == AlgorithmType.MACHINE_SELECTOR:
            return self._generate_machine_output(input_data)
        elif self.algorithm_type == AlgorithmType.CAPACITY_CALCULATOR:
            return self._generate_capacity_output(input_data)
        else:
            return input_data  # ç®€å•ä¼ é€’
    
    def _generate_calendar_output(self, input_data: Any) -> Dict[str, Any]:
        """ç”Ÿæˆæ—¥å†æœåŠ¡è¾“å‡º"""
        return {
            "working_days": 22,
            "holidays": ["2024-12-25", "2024-12-26"],
            "total_working_hours": 176,
            "calendar_constraints": {
                "maintenance_windows": [],
                "shift_schedules": SHIFT_CONFIG
            }
        }
    
    def _generate_machine_output(self, input_data: Any) -> Dict[str, Any]:
        """ç”Ÿæˆæœºå°é€‰æ‹©è¾“å‡º"""
        return {
            "selected_feeders": ["F101", "F102", "F103"],
            "selected_makers": ["M201", "M202", "M203"],
            "machine_assignments": [
                {"plan_id": 1, "feeder": "F101", "maker": "M201"},
                {"plan_id": 2, "feeder": "F102", "maker": "M202"},
                {"plan_id": 3, "feeder": "F103", "maker": "M203"}
            ],
            "optimization_score": 0.85
        }
    
    def _generate_capacity_output(self, input_data: Any) -> Dict[str, Any]:
        """ç”Ÿæˆå®¹é‡è®¡ç®—è¾“å‡º"""
        return {
            "total_capacity": 2400.0,
            "utilized_capacity": 2040.0,
            "capacity_utilization": 0.85,
            "capacity_by_machine": {
                "F101": 800.0, "F102": 800.0, "F103": 800.0,
                "M201": 680.0, "M202": 680.0, "M203": 680.0
            },
            "bottleneck_analysis": {
                "bottleneck_machine": "M201",
                "bottleneck_factor": 0.85
            }
        }


# =============================================================================
# æœˆåº¦ç®—æ³•ç®¡é“é›†æˆæµ‹è¯•ç±»
# =============================================================================

class TestMonthlyAlgorithmPipelineIntegration:
    """æœˆåº¦ç®—æ³•ç®¡é“æ‰§è¡Œé›†æˆæµ‹è¯•"""
    
    def setup_method(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.performance_monitor = PerformanceMonitor()
        self.test_reporter = TestReporter()
        self.concurrent_manager = ConcurrentTestManager(max_workers=3)
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            "max_execution_time": 300,  # 5åˆ†é’Ÿè¶…æ—¶
            "memory_limit_mb": 512,     # 512MBå†…å­˜é™åˆ¶
            "concurrent_plans": 10,     # å¹¶å‘å¤„ç†è®¡åˆ’æ•°
            "performance_threshold": {
                "execution_time": 30.0,    # 30ç§’æ‰§è¡Œé˜ˆå€¼
                "memory_usage": 256.0,     # 256MBå†…å­˜é˜ˆå€¼
                "throughput": 50.0          # 50è®¡åˆ’/ç§’ååé‡é˜ˆå€¼
            }
        }
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        self.test_plans = self._create_test_monthly_plans()
        self.test_machines = self._create_test_machines()
        self.test_calendar = self._create_test_calendar()
        
        # ç®—æ³•ç®¡é“æ¨¡æ‹Ÿå™¨
        self.pipeline_simulators = self._create_pipeline_simulators()
        
    def _create_test_monthly_plans(self) -> List[MonthlyPlanItem]:
        """åˆ›å»ºæµ‹è¯•æœˆåº¦è®¡åˆ’æ•°æ®"""
        plans = []
        base_date = datetime(2024, 12, 1)
        
        # ç”Ÿæˆå¤šç§ç±»å‹çš„è®¡åˆ’
        plan_configs = [
            ("HZWO202412001", "HNZJHYLC001", "åˆ©ç¾¤ï¼ˆé˜³å…‰ï¼‰", 1200.0, 100, ["F101"], ["M201"], Priority.HIGH),
            ("HZWO202412002", "HNZJHYLC002", "åˆ©ç¾¤ï¼ˆæ–°ç‰ˆï¼‰", 800.0, 80, ["F102"], ["M202"], Priority.MEDIUM),
            ("HZWO202412003", "HNZJHYLC003", "åˆ©ç¾¤ï¼ˆä¼‘é—²ï¼‰", 1500.0, 125, ["F103"], ["M203"], Priority.HIGH),
            ("HZWO202412004", "HNZJHYLC004", "é»„é‡‘å¶ï¼ˆç¡¬ï¼‰", 900.0, 75, ["F101", "F102"], ["M201"], Priority.MEDIUM),
            ("HZWO202412005", "HNZJHYLC005", "é»„é‡‘å¶ï¼ˆè½¯ï¼‰", 1100.0, 90, ["F103"], ["M202", "M203"], Priority.LOW),
            ("HZWO202412006", "HNZJHYLC006", "çœŸé¾™ï¼ˆç¡¬ï¼‰", 700.0, 60, ["F101"], ["M203"], Priority.URGENT),
            ("HZWO202412007", "HNZJHYLC007", "çœŸé¾™ï¼ˆè½¯ï¼‰", 1300.0, 110, ["F102", "F103"], ["M201", "M202"], Priority.HIGH),
            ("HZWO202412008", "HNZJHYLC008", "å¤§é‡ä¹", 600.0, 50, ["F101"], ["M201"], Priority.MEDIUM),
            ("HZWO202412009", "HNZJHYLC009", "è¥¿æ¹–ï¼ˆç¡¬ï¼‰", 1000.0, 85, ["F102"], ["M202"], Priority.LOW),
            ("HZWO202412010", "HNZJHYLC010", "è¥¿æ¹–ï¼ˆè½¯ï¼‰", 1400.0, 120, ["F103"], ["M203"], Priority.HIGH)
        ]
        
        for i, (work_order, article_nr, name, quantity, boxes, feeders, makers, priority) in enumerate(plan_configs):
            plan = MonthlyPlanItem(
                plan_id=i + 1,
                batch_id=f"MONTHLY_BATCH_2024_12_{i+1:03d}",
                work_order_nr=work_order,
                article_nr=article_nr,
                article_name=name,
                target_quantity=quantity,
                planned_boxes=boxes,
                feeder_codes=feeders,
                maker_codes=makers,
                planned_start=base_date + timedelta(days=i*2),
                planned_end=base_date + timedelta(days=i*2 + 3),
                priority=priority
            )
            plans.append(plan)
        
        return plans
    
    def _create_test_machines(self) -> List[MachineInfo]:
        """åˆ›å»ºæµ‹è¯•æœºå°ä¿¡æ¯"""
        machines = []
        base_time = datetime(2024, 12, 1, 8, 0)
        
        # å–‚ä¸æœº
        feeder_configs = [
            ("F101", 150.0, 0.95),
            ("F102", 140.0, 0.90),
            ("F103", 160.0, 0.92)
        ]
        
        for code, capacity, efficiency in feeder_configs:
            machine = MachineInfo(
                machine_code=code,
                machine_type=MachineType.FEEDER,
                capacity_per_hour=capacity,
                efficiency_factor=efficiency,
                maintenance_windows=[
                    (base_time + timedelta(days=7), base_time + timedelta(days=7, hours=2)),
                    (base_time + timedelta(days=14), base_time + timedelta(days=14, hours=1))
                ],
                is_available=True
            )
            machines.append(machine)
        
        # å·åŒ…æœº
        maker_configs = [
            ("M201", 120.0, 0.88),
            ("M202", 130.0, 0.91),
            ("M203", 125.0, 0.89)
        ]
        
        for code, capacity, efficiency in maker_configs:
            machine = MachineInfo(
                machine_code=code,
                machine_type=MachineType.MAKER,
                capacity_per_hour=capacity,
                efficiency_factor=efficiency,
                maintenance_windows=[
                    (base_time + timedelta(days=10), base_time + timedelta(days=10, hours=3)),
                    (base_time + timedelta(days=20), base_time + timedelta(days=20, hours=2))
                ],
                is_available=True
            )
            machines.append(machine)
        
        return machines
    
    def _create_test_calendar(self) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•å·¥ä½œæ—¥å†"""
        base_date = datetime(2024, 12, 1)
        return {
            "year": 2024,
            "month": 12,
            "working_days": [
                base_date + timedelta(days=i) 
                for i in range(31) 
                if (base_date + timedelta(days=i)).weekday() < 5
            ],
            "holidays": [
                datetime(2024, 12, 25),  # åœ£è¯èŠ‚
                datetime(2024, 12, 26)   # èŠ‚ç¤¼æ—¥
            ],
            "maintenance_days": [
                datetime(2024, 12, 15)   # è®¾å¤‡ç»´æŠ¤æ—¥
            ],
            "shift_config": SHIFT_CONFIG,
            "total_working_hours": 176,  # 22ä¸ªå·¥ä½œæ—¥ * 8å°æ—¶
            "overtime_limit": 2.0        # æ¯æ—¥æœ€å¤§åŠ ç­2å°æ—¶
        }
    
    def _create_pipeline_simulators(self) -> Dict[str, MockAlgorithmModule]:
        """åˆ›å»ºç®¡é“ç®—æ³•æ¨¡æ‹Ÿå™¨"""
        simulators = {}
        
        # æ ¹æ®ç®—æ³•ç®¡é“é¡ºåºåˆ›å»ºæ¨¡æ‹Ÿå™¨
        algorithm_timings = {
            "calendar_service": 0.2,
            "monthly_capacity_calculator": 0.5,
            "capacity_calculator": 0.3,
            "machine_selector": 0.8,
            "time_allocator": 0.6,
            "constraint_solver": 1.2,
            "load_balancer": 0.4,
            "monthly_engine": 0.3
        }
        
        for alg_name in ALGORITHM_PIPELINE:
            if alg_name in algorithm_timings:
                # è·å–å¯¹åº”çš„ç®—æ³•ç±»å‹
                algorithm_type = getattr(AlgorithmType, alg_name.upper(), AlgorithmType.MONTHLY_ENGINE)
                processing_time = algorithm_timings[alg_name]
                
                simulator = MockAlgorithmModule(algorithm_type, processing_time)
                simulators[alg_name] = simulator
        
        return simulators

    def test_complete_algorithm_pipeline_execution(self):
        """æµ‹è¯•å®Œæ•´ç®—æ³•ç®¡é“æ‰§è¡Œ - ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•"""
        print("\nğŸš€ å¼€å§‹æœˆåº¦ç®—æ³•ç®¡é“å®Œæ•´æ‰§è¡Œé›†æˆæµ‹è¯•")
        
        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        self.performance_monitor.start_monitoring()
        start_time = time.time()
        
        try:
            # æ‰§è¡Œå®Œæ•´ç®¡é“
            pipeline_result = asyncio.run(self._execute_full_pipeline())
            
            # åœæ­¢æ€§èƒ½ç›‘æ§
            execution_time = time.time() - start_time
            performance_metrics = self.performance_monitor.stop_monitoring(
                records_processed=len(self.test_plans)
            )
            
            # éªŒè¯æ‰§è¡Œç»“æœ
            assert pipeline_result.success, f"ç®¡é“æ‰§è¡Œå¤±è´¥: {pipeline_result.errors}"
            assert pipeline_result.processed_plans == len(self.test_plans)
            assert pipeline_result.generated_schedules > 0
            
            # æ€§èƒ½éªŒè¯
            assert execution_time < self.test_config["performance_threshold"]["execution_time"]
            assert performance_metrics.memory_usage_mb < self.test_config["performance_threshold"]["memory_usage"]
            
            # æ•°æ®ä¸€è‡´æ€§éªŒè¯
            self._verify_pipeline_data_consistency(pipeline_result)
            
            # è®°å½•æµ‹è¯•ç»“æœ
            self.test_reporter.add_test_result(
                "complete_pipeline_execution",
                True,
                {
                    "processed_plans": pipeline_result.processed_plans,
                    "generated_schedules": pipeline_result.generated_schedules,
                    "execution_time": execution_time,
                    "algorithm_steps": len(ALGORITHM_PIPELINE)
                },
                performance_metrics
            )
            
            print(f"âœ… ç®¡é“æ‰§è¡ŒæˆåŠŸ")
            print(f"ğŸ“Š å¤„ç†è®¡åˆ’: {pipeline_result.processed_plans}")
            print(f"ğŸ“‹ ç”Ÿæˆæ’ç¨‹: {pipeline_result.generated_schedules}")
            print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
            print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {performance_metrics.memory_usage_mb:.1f}MB")
            
        except Exception as e:
            pytest.fail(f"ç®¡é“æ‰§è¡Œæµ‹è¯•å¤±è´¥: {str(e)}")

    async def _execute_full_pipeline(self) -> PipelineExecutionResult:
        """æ‰§è¡Œå®Œæ•´ç®—æ³•ç®¡é“"""
        algorithm_results = {}
        errors = []
        warnings = []
        total_schedules = 0
        
        # åˆå§‹åŒ–è¾“å…¥æ•°æ®
        pipeline_data = {
            "monthly_plans": [plan.to_dict() for plan in self.test_plans],
            "machines": [machine.to_dict() for machine in self.test_machines],
            "calendar": self.test_calendar,
            "config": ALGORITHM_CONFIG
        }
        
        try:
            # æŒ‰é¡ºåºæ‰§è¡Œç®—æ³•ç®¡é“
            for algorithm_name in ALGORITHM_PIPELINE:
                if algorithm_name in self.pipeline_simulators:
                    print(f"  ğŸ”„ æ‰§è¡Œç®—æ³•: {algorithm_name}")
                    
                    simulator = self.pipeline_simulators[algorithm_name]
                    step_start = time.time()
                    
                    # æ‰§è¡Œç®—æ³•æ­¥éª¤
                    step_result = await simulator.execute(pipeline_data)
                    step_time = time.time() - step_start
                    
                    # è®°å½•æ­¥éª¤ç»“æœ
                    algorithm_results[algorithm_name] = {
                        "result": step_result,
                        "execution_time": step_time,
                        "input_size": len(str(pipeline_data)),
                        "output_size": len(str(step_result))
                    }
                    
                    # æ›´æ–°ç®¡é“æ•°æ®
                    pipeline_data[f"{algorithm_name}_result"] = step_result
                    
                    # æ¨¡æ‹Ÿç”Ÿæˆæ’ç¨‹æ•°æ®
                    if algorithm_name == "monthly_engine":
                        total_schedules = len(self.test_plans)
                    
                    print(f"    âœ… {algorithm_name} å®Œæˆ ({step_time:.2f}ç§’)")
                else:
                    warnings.append(f"ç®—æ³•æ¨¡å— {algorithm_name} æœªå®ç°")
            
            return PipelineExecutionResult(
                success=True,
                execution_time=sum(r["execution_time"] for r in algorithm_results.values()),
                processed_plans=len(self.test_plans),
                generated_schedules=total_schedules,
                memory_usage_mb=0.0,  # å°†ç”±å¤–éƒ¨ç›‘æ§å™¨å¡«å……
                algorithm_results=algorithm_results,
                errors=errors,
                warnings=warnings,
                performance_metrics={}
            )
            
        except Exception as e:
            errors.append(str(e))
            return PipelineExecutionResult(
                success=False,
                execution_time=0.0,
                processed_plans=0,
                generated_schedules=0,
                memory_usage_mb=0.0,
                algorithm_results=algorithm_results,
                errors=errors,
                warnings=warnings,
                performance_metrics={}
            )

    def test_algorithm_dependencies_and_data_flow(self):
        """æµ‹è¯•ç®—æ³•é—´ä¾èµ–å…³ç³»å’Œæ•°æ®æµè½¬"""
        print("\nğŸ”— æµ‹è¯•ç®—æ³•ä¾èµ–å…³ç³»å’Œæ•°æ®æµè½¬")
        
        try:
            # éªŒè¯ç®—æ³•ç®¡é“é¡ºåº
            pipeline_order = get_pipeline_order()
            assert len(pipeline_order) == len(ALGORITHM_PIPELINE)
            assert pipeline_order == ALGORITHM_PIPELINE
            
            # éªŒè¯ç®—æ³•æ¨¡å—ä¿¡æ¯
            for algorithm_name in ALGORITHM_PIPELINE:
                module_info = get_algorithm_info(algorithm_name)
                assert module_info is not None, f"ç®—æ³•æ¨¡å— {algorithm_name} ä¿¡æ¯ç¼ºå¤±"
                assert "name" in module_info
                assert "description" in module_info
                assert "dependencies" in module_info
            
            # æµ‹è¯•æ•°æ®æµè½¬
            asyncio.run(self._test_data_flow_integrity())
            
            print("âœ… ç®—æ³•ä¾èµ–å…³ç³»éªŒè¯é€šè¿‡")
            print("âœ… æ•°æ®æµè½¬å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            
        except Exception as e:
            pytest.fail(f"ä¾èµ–å…³ç³»æµ‹è¯•å¤±è´¥: {str(e)}")

    async def _test_data_flow_integrity(self):
        """æµ‹è¯•æ•°æ®æµè½¬å®Œæ•´æ€§"""
        initial_data = {
            "plans": self.test_plans[:3],  # ä½¿ç”¨å‰3ä¸ªè®¡åˆ’è¿›è¡Œæµ‹è¯•
            "machines": self.test_machines[:2],
            "calendar": self.test_calendar
        }
        
        current_data = initial_data.copy()
        
        # é€æ­¥æ‰§è¡Œç®—æ³•å¹¶éªŒè¯æ•°æ®å®Œæ•´æ€§
        for i, algorithm_name in enumerate(ALGORITHM_PIPELINE[:4]):  # æµ‹è¯•å‰4ä¸ªç®—æ³•
            if algorithm_name in self.pipeline_simulators:
                simulator = self.pipeline_simulators[algorithm_name]
                
                # æ‰§è¡Œç®—æ³•
                result = await simulator.execute(current_data)
                
                # éªŒè¯è¾“å‡ºæ•°æ®ç»“æ„
                assert isinstance(result, dict), f"{algorithm_name} è¾“å‡ºä¸æ˜¯å­—å…¸æ ¼å¼"
                assert len(result) > 0, f"{algorithm_name} è¾“å‡ºä¸ºç©º"
                
                # æ›´æ–°å½“å‰æ•°æ®
                current_data[f"step_{i+1}_result"] = result
                
                print(f"  âœ“ {algorithm_name} æ•°æ®æµè½¬æ­£å¸¸")

    def test_parallel_execution_and_performance(self):
        """æµ‹è¯•å¹¶è¡Œæ‰§è¡Œå’Œæ€§èƒ½ä¼˜åŒ–"""
        print("\nâš¡ æµ‹è¯•å¹¶è¡Œæ‰§è¡Œå’Œæ€§èƒ½ä¼˜åŒ–")
        
        # æµ‹è¯•ä¸åŒè§„æ¨¡çš„å¹¶å‘æ‰§è¡Œ
        test_scales = [1, 3, 5, 10]
        performance_results = []
        
        for scale in test_scales:
            print(f"  ğŸ“Š æµ‹è¯•å¹¶å‘è§„æ¨¡: {scale}")
            
            # åˆ›å»ºå¤šä¸ªè®¡åˆ’æ‰¹æ¬¡
            test_batches = [
                self.test_plans[i:i+2] if i+2 <= len(self.test_plans) else self.test_plans[i:]
                for i in range(0, min(scale*2, len(self.test_plans)), 2)
            ]
            
            if not test_batches:
                continue
            
            # æ€§èƒ½ç›‘æ§
            monitor = PerformanceMonitor()
            monitor.start_monitoring()
            start_time = time.time()
            
            try:
                # å¹¶å‘æ‰§è¡Œ
                results = asyncio.run(self._execute_concurrent_pipelines(test_batches))
                
                execution_time = time.time() - start_time
                metrics = monitor.stop_monitoring(
                    records_processed=sum(len(batch) for batch in test_batches)
                )
                
                # è®°å½•æ€§èƒ½ç»“æœ
                performance_results.append({
                    "scale": scale,
                    "execution_time": execution_time,
                    "throughput": len(test_batches) / execution_time if execution_time > 0 else 0,
                    "memory_usage": metrics.memory_usage_mb,
                    "cpu_usage": metrics.cpu_usage_percent,
                    "success_rate": sum(1 for r in results if r["success"]) / len(results)
                })
                
                print(f"    âœ… è§„æ¨¡ {scale}: {execution_time:.2f}ç§’, {metrics.memory_usage_mb:.1f}MB")
                
            except Exception as e:
                print(f"    âŒ è§„æ¨¡ {scale} æµ‹è¯•å¤±è´¥: {str(e)}")
        
        # éªŒè¯æ€§èƒ½è¶‹åŠ¿
        if len(performance_results) >= 2:
            # éªŒè¯ååé‡éšè§„æ¨¡çš„å˜åŒ–
            throughputs = [r["throughput"] for r in performance_results]
            assert max(throughputs) > 0, "å¹¶å‘æ‰§è¡Œæ— ååé‡"
            
            # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†æ€§
            memory_usages = [r["memory_usage"] for r in performance_results]
            assert all(m < self.test_config["memory_limit_mb"] for m in memory_usages), "å†…å­˜ä½¿ç”¨è¶…é™"
            
            print(f"âœ… å¹¶è¡Œæ€§èƒ½æµ‹è¯•é€šè¿‡ï¼Œæœ€å¤§ååé‡: {max(throughputs):.1f}æ‰¹æ¬¡/ç§’")

    async def _execute_concurrent_pipelines(self, plan_batches: List[List[MonthlyPlanItem]]) -> List[Dict[str, Any]]:
        """å¹¶å‘æ‰§è¡Œå¤šä¸ªç®¡é“"""
        tasks = []
        
        for i, batch in enumerate(plan_batches):
            # ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºç‹¬ç«‹çš„æ‰§è¡Œä»»åŠ¡
            task_data = {
                "batch_id": f"concurrent_batch_{i}",
                "plans": batch,
                "machines": self.test_machines,
                "calendar": self.test_calendar
            }
            
            task = self._execute_pipeline_batch(task_data)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "batch_id": f"concurrent_batch_{i}",
                    "success": False,
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results

    async def _execute_pipeline_batch(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªç®¡é“æ‰¹æ¬¡"""
        try:
            batch_id = task_data["batch_id"]
            plans = task_data["plans"]
            
            # æ¨¡æ‹Ÿç®—æ³•æ‰§è¡Œ
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            
            # æ¨¡æ‹Ÿç”Ÿæˆç»“æœ
            schedules = [
                ScheduleResult(
                    schedule_id=generate_schedule_id(plan.plan_id),
                    plan_id=plan.plan_id,
                    assigned_feeder=plan.feeder_codes[0] if plan.feeder_codes else "F101",
                    assigned_maker=plan.maker_codes[0] if plan.maker_codes else "M201",
                    scheduled_start=plan.planned_start or datetime.now(),
                    scheduled_end=plan.planned_end or datetime.now() + timedelta(hours=8),
                    allocated_quantity=plan.target_quantity,
                    estimated_duration=8.0,
                    priority_score=calculate_priority_score(plan),
                    constraints_satisfied=True
                )
                for plan in plans
            ]
            
            return {
                "batch_id": batch_id,
                "success": True,
                "processed_plans": len(plans),
                "generated_schedules": len(schedules),
                "schedules": [s.to_dict() for s in schedules]
            }
            
        except Exception as e:
            return {
                "batch_id": task_data.get("batch_id", "unknown"),
                "success": False,
                "error": str(e)
            }

    def test_constraint_compliance_and_validation(self):
        """æµ‹è¯•çº¦æŸéµå¾ªå’Œç»“æœéªŒè¯"""
        print("\nğŸ”’ æµ‹è¯•çº¦æŸéµå¾ªå’Œç»“æœéªŒè¯")
        
        # å®šä¹‰æµ‹è¯•çº¦æŸ
        test_constraints = {
            "working_hours": {
                "max_daily_hours": 16,
                "max_weekly_hours": 80,
                "min_break_time": 1.0
            },
            "machine_capacity": {
                "max_utilization": 0.95,
                "min_efficiency": 0.80
            },
            "production_rules": {
                "max_continuous_production": 12,  # å°æ—¶
                "min_setup_time": 0.5,           # å°æ—¶
                "max_batch_size": 200.0           # äº§é‡
            },
            "calendar_rules": {
                "respect_holidays": True,
                "respect_maintenance": True,
                "allow_overtime": True
            }
        }
        
        try:
            # æ‰§è¡Œçº¦æŸéªŒè¯æµ‹è¯•
            validation_results = asyncio.run(
                self._validate_constraint_compliance(test_constraints)
            )
            
            # éªŒè¯ç»“æœ
            assert validation_results["overall_compliance"], "æ•´ä½“çº¦æŸéµå¾ªå¤±è´¥"
            assert validation_results["working_hours_compliance"], "å·¥ä½œæ—¶é—´çº¦æŸè¿å"
            assert validation_results["capacity_compliance"], "äº§èƒ½çº¦æŸè¿å"
            assert validation_results["calendar_compliance"], "æ—¥å†çº¦æŸè¿å"
            
            # æ£€æŸ¥è¿è§„è¯¦æƒ…
            violations = validation_results.get("violations", [])
            critical_violations = [v for v in violations if v.get("severity") == "critical"]
            assert len(critical_violations) == 0, f"å‘ç°ä¸¥é‡çº¦æŸè¿è§„: {critical_violations}"
            
            print("âœ… çº¦æŸéµå¾ªéªŒè¯é€šè¿‡")
            print(f"ğŸ“Š æ€»ä½“åˆè§„ç‡: {validation_results.get('compliance_rate', 0):.1%}")
            print(f"âš ï¸ è­¦å‘Šæ•°é‡: {len([v for v in violations if v.get('severity') == 'warning'])}")
            
        except Exception as e:
            pytest.fail(f"çº¦æŸéªŒè¯æµ‹è¯•å¤±è´¥: {str(e)}")

    async def _validate_constraint_compliance(self, constraints: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯çº¦æŸéµå¾ªæƒ…å†µ"""
        violations = []
        
        # æ¨¡æ‹Ÿçº¦æŸæ£€æŸ¥
        working_hours_ok = True
        capacity_ok = True
        calendar_ok = True
        
        # éªŒè¯å·¥ä½œæ—¶é—´çº¦æŸ
        for plan in self.test_plans[:5]:  # æµ‹è¯•å‰5ä¸ªè®¡åˆ’
            if plan.planned_start and plan.planned_end:
                duration = (plan.planned_end - plan.planned_start).total_seconds() / 3600
                if duration > constraints["working_hours"]["max_daily_hours"]:
                    violations.append({
                        "type": "working_hours",
                        "severity": "critical",
                        "plan_id": plan.plan_id,
                        "message": f"è®¡åˆ’æŒç»­æ—¶é—´ {duration:.1f}h è¶…è¿‡é™åˆ¶"
                    })
                    working_hours_ok = False
        
        # éªŒè¯æœºå°å®¹é‡çº¦æŸ
        for machine in self.test_machines:
            utilization = machine.efficiency_factor
            if utilization > constraints["machine_capacity"]["max_utilization"]:
                violations.append({
                    "type": "capacity",
                    "severity": "warning",
                    "machine": machine.machine_code,
                    "message": f"æœºå°åˆ©ç”¨ç‡ {utilization:.1%} è¶…è¿‡æ¨èå€¼"
                })
        
        # éªŒè¯æ—¥å†çº¦æŸ
        if constraints["calendar_rules"]["respect_holidays"]:
            for holiday in self.test_calendar["holidays"]:
                # æ£€æŸ¥æ˜¯å¦æœ‰è®¡åˆ’å®‰æ’åœ¨å‡æœŸ
                holiday_plans = [
                    p for p in self.test_plans 
                    if p.planned_start and p.planned_start.date() == holiday.date()
                ]
                if holiday_plans:
                    violations.append({
                        "type": "calendar",
                        "severity": "critical",
                        "date": holiday.isoformat(),
                        "message": f"å‡æœŸ {holiday.date()} å®‰æ’äº† {len(holiday_plans)} ä¸ªè®¡åˆ’"
                    })
                    calendar_ok = False
        
        # è®¡ç®—åˆè§„ç‡
        total_checks = len(self.test_plans) + len(self.test_machines) + len(self.test_calendar["holidays"])
        compliance_rate = 1.0 - (len(violations) / total_checks) if total_checks > 0 else 1.0
        
        return {
            "overall_compliance": len([v for v in violations if v["severity"] == "critical"]) == 0,
            "working_hours_compliance": working_hours_ok,
            "capacity_compliance": capacity_ok,
            "calendar_compliance": calendar_ok,
            "compliance_rate": compliance_rate,
            "violations": violations,
            "total_checks": total_checks
        }

    def test_error_handling_and_recovery(self):
        """æµ‹è¯•å¼‚å¸¸å¤„ç†å’Œé”™è¯¯æ¢å¤"""
        print("\nğŸš¨ æµ‹è¯•å¼‚å¸¸å¤„ç†å’Œé”™è¯¯æ¢å¤")
        
        # å®šä¹‰é”™è¯¯åœºæ™¯
        error_scenarios = [
            {
                "name": "ç®—æ³•è¶…æ—¶",
                "error_type": AlgorithmTimeoutError,
                "description": "æ¨¡æ‹Ÿç®—æ³•æ‰§è¡Œè¶…æ—¶"
            },
            {
                "name": "çº¦æŸè¿å",
                "error_type": ConstraintViolationError,
                "description": "æ¨¡æ‹Ÿçº¦æŸæ¡ä»¶è¿å"
            },
            {
                "name": "èµ„æºå†²çª",
                "error_type": ResourceConflictError,
                "description": "æ¨¡æ‹Ÿæœºå°èµ„æºå†²çª"
            },
            {
                "name": "æ•°æ®æ— æ•ˆ",
                "error_type": ValueError,
                "description": "æ¨¡æ‹Ÿè¾“å…¥æ•°æ®æ— æ•ˆ"
            },
            {
                "name": "å†…å­˜ä¸è¶³",
                "error_type": MemoryError,
                "description": "æ¨¡æ‹Ÿå†…å­˜ä¸è¶³æƒ…å†µ"
            }
        ]
        
        error_handling_results = []
        
        for scenario in error_scenarios:
            print(f"  ğŸ§ª æµ‹è¯•åœºæ™¯: {scenario['name']}")
            
            try:
                # æ¨¡æ‹Ÿé”™è¯¯åœºæ™¯
                result = asyncio.run(
                    self._simulate_error_scenario(scenario)
                )
                
                error_handling_results.append({
                    "scenario": scenario["name"],
                    "handled": result["error_handled"],
                    "recovery": result["recovery_successful"],
                    "cleanup": result["cleanup_completed"],
                    "error_message": result.get("error_message")
                })
                
                print(f"    âœ… é”™è¯¯å¤„ç†: {'æˆåŠŸ' if result['error_handled'] else 'å¤±è´¥'}")
                print(f"    ğŸ”„ é”™è¯¯æ¢å¤: {'æˆåŠŸ' if result['recovery_successful'] else 'å¤±è´¥'}")
                
            except Exception as e:
                print(f"    âŒ åœºæ™¯æµ‹è¯•å¤±è´¥: {str(e)}")
                error_handling_results.append({
                    "scenario": scenario["name"],
                    "handled": False,
                    "recovery": False,
                    "cleanup": False,
                    "error_message": str(e)
                })
        
        # éªŒè¯é”™è¯¯å¤„ç†èƒ½åŠ›
        successful_handling = sum(1 for r in error_handling_results if r["handled"])
        total_scenarios = len(error_scenarios)
        
        assert successful_handling >= total_scenarios * 0.8, \
            f"é”™è¯¯å¤„ç†æˆåŠŸç‡è¿‡ä½: {successful_handling}/{total_scenarios}"
        
        print(f"âœ… é”™è¯¯å¤„ç†æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸç‡: {successful_handling}/{total_scenarios}")

    async def _simulate_error_scenario(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿé”™è¯¯åœºæ™¯"""
        error_handled = False
        recovery_successful = False
        cleanup_completed = False
        error_message = None
        
        try:
            # æ ¹æ®åœºæ™¯ç±»å‹æ¨¡æ‹Ÿä¸åŒé”™è¯¯
            if scenario["error_type"] == AlgorithmTimeoutError:
                # æ¨¡æ‹Ÿè¶…æ—¶é”™è¯¯
                await asyncio.sleep(0.1)
                raise AlgorithmTimeoutError("ç®—æ³•æ‰§è¡Œè¶…æ—¶")
                
            elif scenario["error_type"] == ConstraintViolationError:
                # æ¨¡æ‹Ÿçº¦æŸè¿å
                raise ConstraintViolationError("æœºå°å®¹é‡çº¦æŸè¿å")
                
            elif scenario["error_type"] == ResourceConflictError:
                # æ¨¡æ‹Ÿèµ„æºå†²çª
                raise ResourceConflictError("æœºå° M201 æ—¶é—´å†²çª")
                
            elif scenario["error_type"] == ValueError:
                # æ¨¡æ‹Ÿæ•°æ®é”™è¯¯
                raise ValueError("è¾“å…¥æ•°æ®æ ¼å¼æ— æ•ˆ")
                
            elif scenario["error_type"] == MemoryError:
                # æ¨¡æ‹Ÿå†…å­˜é”™è¯¯
                raise MemoryError("å†…å­˜ä¸è¶³")
                
        except MonthlySchedulingError as e:
            # å¤„ç†ä¸šåŠ¡å¼‚å¸¸
            error_handled = True
            error_message = str(e)
            
            # å°è¯•æ¢å¤
            try:
                # æ¨¡æ‹Ÿé”™è¯¯æ¢å¤é€»è¾‘
                await asyncio.sleep(0.05)
                recovery_successful = True
            except:
                recovery_successful = False
                
        except Exception as e:
            # å¤„ç†å…¶ä»–å¼‚å¸¸
            error_handled = True
            error_message = str(e)
            recovery_successful = False
            
        finally:
            # æ¸…ç†èµ„æº
            try:
                await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæ¸…ç†æ“ä½œ
                cleanup_completed = True
            except:
                cleanup_completed = False
        
        return {
            "error_handled": error_handled,
            "recovery_successful": recovery_successful,
            "cleanup_completed": cleanup_completed,
            "error_message": error_message
        }

    def test_memory_and_performance_benchmarks(self):
        """æµ‹è¯•å†…å­˜å’Œæ€§èƒ½åŸºå‡†"""
        print("\nğŸ“Š æµ‹è¯•å†…å­˜å’Œæ€§èƒ½åŸºå‡†")
        
        # æ€§èƒ½åŸºå‡†é…ç½®
        benchmark_configs = [
            {"name": "å°è§„æ¨¡", "plans": 5, "expected_time": 2.0, "expected_memory": 50},
            {"name": "ä¸­è§„æ¨¡", "plans": 10, "expected_time": 5.0, "expected_memory": 100},
            {"name": "å¤§è§„æ¨¡", "plans": 20, "expected_time": 10.0, "expected_memory": 200}
        ]
        
        benchmark_results = []
        
        for config in benchmark_configs:
            print(f"  ğŸ¯ åŸºå‡†æµ‹è¯•: {config['name']} ({config['plans']}è®¡åˆ’)")
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_plans = self.test_plans[:config["plans"]]
            
            # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
            monitor = PerformanceMonitor()
            monitor.start_monitoring()
            start_time = time.time()
            
            try:
                # æ‰§è¡Œç®—æ³•ç®¡é“
                result = asyncio.run(self._execute_benchmark_pipeline(test_plans))
                
                execution_time = time.time() - start_time
                metrics = monitor.stop_monitoring(records_processed=len(test_plans))
                
                # è®°å½•åŸºå‡†ç»“æœ
                benchmark_result = {
                    "config": config["name"],
                    "plans_count": len(test_plans),
                    "execution_time": execution_time,
                    "memory_usage_mb": metrics.memory_usage_mb,
                    "memory_peak_mb": metrics.memory_peak_mb,
                    "cpu_usage_percent": metrics.cpu_usage_percent,
                    "throughput": len(test_plans) / execution_time if execution_time > 0 else 0,
                    "meets_time_target": execution_time <= config["expected_time"],
                    "meets_memory_target": metrics.memory_usage_mb <= config["expected_memory"]
                }
                
                benchmark_results.append(benchmark_result)
                
                print(f"    â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.2f}s (ç›®æ ‡: {config['expected_time']}s)")
                print(f"    ğŸ’¾ å†…å­˜ä½¿ç”¨: {metrics.memory_usage_mb:.1f}MB (ç›®æ ‡: {config['expected_memory']}MB)")
                print(f"    ğŸš€ ååé‡: {benchmark_result['throughput']:.1f}è®¡åˆ’/ç§’")
                print(f"    âœ… æ€§èƒ½è¾¾æ ‡: {'æ˜¯' if benchmark_result['meets_time_target'] and benchmark_result['meets_memory_target'] else 'å¦'}")
                
            except Exception as e:
                print(f"    âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
                benchmark_results.append({
                    "config": config["name"],
                    "error": str(e),
                    "meets_time_target": False,
                    "meets_memory_target": False
                })
        
        # éªŒè¯åŸºå‡†ç»“æœ
        successful_benchmarks = [r for r in benchmark_results if r.get("meets_time_target", False) and r.get("meets_memory_target", False)]
        
        assert len(successful_benchmarks) >= len(benchmark_configs) * 0.7, \
            f"æ€§èƒ½åŸºå‡†è¾¾æ ‡ç‡è¿‡ä½: {len(successful_benchmarks)}/{len(benchmark_configs)}"
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        self._generate_performance_report(benchmark_results)
        
        print(f"âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼Œè¾¾æ ‡ç‡: {len(successful_benchmarks)}/{len(benchmark_configs)}")

    async def _execute_benchmark_pipeline(self, test_plans: List[MonthlyPlanItem]) -> Dict[str, Any]:
        """æ‰§è¡ŒåŸºå‡†æµ‹è¯•ç®¡é“"""
        # åˆ›å»ºç²¾ç®€çš„ç®¡é“æ•°æ®
        pipeline_data = {
            "plans": [plan.to_dict() for plan in test_plans],
            "machines": [machine.to_dict() for machine in self.test_machines[:3]],  # åªä½¿ç”¨3å°æœºå™¨
            "calendar": self.test_calendar
        }
        
        # æ‰§è¡Œæ ¸å¿ƒç®—æ³•æ­¥éª¤
        core_algorithms = ["calendar_service", "machine_selector", "capacity_calculator", "monthly_engine"]
        
        for alg_name in core_algorithms:
            if alg_name in self.pipeline_simulators:
                simulator = self.pipeline_simulators[alg_name]
                result = await simulator.execute(pipeline_data)
                pipeline_data[f"{alg_name}_result"] = result
        
        return {
            "processed_plans": len(test_plans),
            "algorithm_steps": len(core_algorithms),
            "success": True
        }

    def test_work_calendar_integration(self):
        """æµ‹è¯•å·¥ä½œæ—¥å†é›†æˆ"""
        print("\nğŸ“… æµ‹è¯•å·¥ä½œæ—¥å†é›†æˆ")
        
        try:
            # æµ‹è¯•æ—¥å†æœåŠ¡åŠŸèƒ½
            calendar_tests = [
                self._test_working_days_calculation(),
                self._test_holiday_handling(),
                self._test_maintenance_window_integration(),
                self._test_shift_schedule_validation(),
                self._test_overtime_calculation()
            ]
            
            results = asyncio.run(asyncio.gather(*calendar_tests))
            
            # éªŒè¯æ‰€æœ‰æµ‹è¯•é€šè¿‡
            all_passed = all(result["success"] for result in results)
            assert all_passed, f"æ—¥å†é›†æˆæµ‹è¯•å¤±è´¥: {[r for r in results if not r['success']]}"
            
            print("âœ… å·¥ä½œæ—¥å†é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡")
            
            # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
            for result in results:
                print(f"  âœ“ {result['test_name']}: {result.get('message', 'æˆåŠŸ')}")
                
        except Exception as e:
            pytest.fail(f"å·¥ä½œæ—¥å†é›†æˆæµ‹è¯•å¤±è´¥: {str(e)}")

    async def _test_working_days_calculation(self) -> Dict[str, Any]:
        """æµ‹è¯•å·¥ä½œæ—¥è®¡ç®—"""
        try:
            calendar = self.test_calendar
            
            # éªŒè¯å·¥ä½œæ—¥æ•°é‡
            working_days = calendar["working_days"]
            expected_working_days = 22  # 12æœˆé¢„æœŸå·¥ä½œæ—¥
            
            assert len(working_days) >= expected_working_days * 0.9, \
                f"å·¥ä½œæ—¥æ•°é‡ä¸è¶³: {len(working_days)}"
            
            # éªŒè¯å·¥ä½œæ—¥ä¸åŒ…å«å‘¨æœ«
            for day in working_days:
                assert day.weekday() < 5, f"å·¥ä½œæ—¥åŒ…å«å‘¨æœ«: {day}"
            
            # éªŒè¯æ€»å·¥ä½œæ—¶é—´
            total_hours = calendar["total_working_hours"]
            expected_hours = len(working_days) * 8  # æ¯å¤©8å°æ—¶
            
            assert abs(total_hours - expected_hours) <= 16, \
                f"æ€»å·¥ä½œæ—¶é—´è®¡ç®—é”™è¯¯: {total_hours} vs {expected_hours}"
            
            return {"success": True, "test_name": "å·¥ä½œæ—¥è®¡ç®—"}
            
        except Exception as e:
            return {"success": False, "test_name": "å·¥ä½œæ—¥è®¡ç®—", "error": str(e)}

    async def _test_holiday_handling(self) -> Dict[str, Any]:
        """æµ‹è¯•å‡æœŸå¤„ç†"""
        try:
            calendar = self.test_calendar
            holidays = calendar["holidays"]
            working_days = calendar["working_days"]
            
            # éªŒè¯å‡æœŸä¸åœ¨å·¥ä½œæ—¥ä¸­
            for holiday in holidays:
                working_dates = [day.date() for day in working_days]
                assert holiday.date() not in working_dates, \
                    f"å‡æœŸ {holiday.date()} è¢«è®¡å…¥å·¥ä½œæ—¥"
            
            # éªŒè¯å‡æœŸæ•°é‡åˆç†
            assert len(holidays) <= 5, f"å‡æœŸæ•°é‡è¿‡å¤š: {len(holidays)}"
            
            return {"success": True, "test_name": "å‡æœŸå¤„ç†"}
            
        except Exception as e:
            return {"success": False, "test_name": "å‡æœŸå¤„ç†", "error": str(e)}

    async def _test_maintenance_window_integration(self) -> Dict[str, Any]:
        """æµ‹è¯•ç»´æŠ¤çª—å£é›†æˆ"""
        try:
            # éªŒè¯æœºå°ç»´æŠ¤çª—å£
            for machine in self.test_machines:
                maintenance_windows = machine.maintenance_windows
                
                # éªŒè¯ç»´æŠ¤çª—å£æ—¶é—´æœ‰æ•ˆæ€§
                for start_time, end_time in maintenance_windows:
                    assert start_time < end_time, \
                        f"æœºå° {machine.machine_code} ç»´æŠ¤çª—å£æ—¶é—´æ— æ•ˆ"
                    
                    # éªŒè¯ç»´æŠ¤æ—¶é—´ä¸è¶…è¿‡24å°æ—¶
                    duration = (end_time - start_time).total_seconds() / 3600
                    assert duration <= 24, \
                        f"æœºå° {machine.machine_code} ç»´æŠ¤æ—¶é—´è¿‡é•¿: {duration}å°æ—¶"
            
            return {"success": True, "test_name": "ç»´æŠ¤çª—å£é›†æˆ"}
            
        except Exception as e:
            return {"success": False, "test_name": "ç»´æŠ¤çª—å£é›†æˆ", "error": str(e)}

    async def _test_shift_schedule_validation(self) -> Dict[str, Any]:
        """æµ‹è¯•ç­æ¬¡å®‰æ’éªŒè¯"""
        try:
            shift_config = SHIFT_CONFIG
            
            # éªŒè¯ç­æ¬¡é…ç½®å®Œæ•´æ€§
            required_shifts = ["day_shift", "night_shift"]
            for shift in required_shifts:
                assert shift in shift_config, f"ç¼ºå°‘ç­æ¬¡é…ç½®: {shift}"
                
                shift_info = shift_config[shift]
                assert "start_time" in shift_info, f"ç­æ¬¡ {shift} ç¼ºå°‘å¼€å§‹æ—¶é—´"
                assert "end_time" in shift_info, f"ç­æ¬¡ {shift} ç¼ºå°‘ç»“æŸæ—¶é—´"
                assert "capacity_factor" in shift_info, f"ç­æ¬¡ {shift} ç¼ºå°‘äº§èƒ½ç³»æ•°"
            
            # éªŒè¯ç­æ¬¡äº§èƒ½ç³»æ•°åˆç†æ€§
            for shift_name, shift_info in shift_config.items():
                factor = shift_info["capacity_factor"]
                assert 0.5 <= factor <= 1.2, \
                    f"ç­æ¬¡ {shift_name} äº§èƒ½ç³»æ•°ä¸åˆç†: {factor}"
            
            return {"success": True, "test_name": "ç­æ¬¡å®‰æ’éªŒè¯"}
            
        except Exception as e:
            return {"success": False, "test_name": "ç­æ¬¡å®‰æ’éªŒè¯", "error": str(e)}

    async def _test_overtime_calculation(self) -> Dict[str, Any]:
        """æµ‹è¯•åŠ ç­æ—¶é—´è®¡ç®—"""
        try:
            calendar = self.test_calendar
            overtime_limit = calendar.get("overtime_limit", 2.0)
            
            # éªŒè¯åŠ ç­é™åˆ¶åˆç†æ€§
            assert 0 <= overtime_limit <= 4, f"åŠ ç­é™åˆ¶ä¸åˆç†: {overtime_limit}å°æ—¶"
            
            # æ¨¡æ‹Ÿè®¡ç®—åŒ…å«åŠ ç­çš„å·¥ä½œæ—¶é—´
            base_hours = 8  # æ ‡å‡†å·¥ä½œæ—¶é—´
            max_daily_hours = base_hours + overtime_limit
            
            # éªŒè¯æœ€å¤§æ—¥å·¥ä½œæ—¶é—´
            assert max_daily_hours <= 12, f"æ¯æ—¥æœ€å¤§å·¥ä½œæ—¶é—´è¿‡é•¿: {max_daily_hours}å°æ—¶"
            
            # æµ‹è¯•å·¥ä½œæ—¶é—´è®¡ç®—å‡½æ•°
            start_time = datetime(2024, 12, 1, 8, 0)
            end_time = datetime(2024, 12, 1, 18, 0)  # 10å°æ—¶
            
            working_hours = calculate_working_hours(start_time, end_time)
            expected_hours = 9.0  # 10å°æ—¶å‡å»1å°æ—¶ä¼‘æ¯
            
            assert abs(working_hours - expected_hours) <= 1.0, \
                f"å·¥ä½œæ—¶é—´è®¡ç®—é”™è¯¯: {working_hours} vs {expected_hours}"
            
            return {"success": True, "test_name": "åŠ ç­æ—¶é—´è®¡ç®—"}
            
        except Exception as e:
            return {"success": False, "test_name": "åŠ ç­æ—¶é—´è®¡ç®—", "error": str(e)}

    def test_real_data_scenario_integration(self):
        """æµ‹è¯•çœŸå®æ•°æ®åœºæ™¯é›†æˆ"""
        print("\nğŸ­ æµ‹è¯•çœŸå®æ•°æ®åœºæ™¯é›†æˆ")
        
        # åˆ›å»ºæ›´çœŸå®çš„æµ‹è¯•åœºæ™¯
        real_scenario = self._create_realistic_scenario()
        
        try:
            # æ‰§è¡ŒçœŸå®åœºæ™¯æµ‹è¯•
            scenario_result = asyncio.run(
                self._execute_realistic_scenario(real_scenario)
            )
            
            # éªŒè¯åœºæ™¯æ‰§è¡Œç»“æœ
            assert scenario_result["success"], f"çœŸå®åœºæ™¯æ‰§è¡Œå¤±è´¥: {scenario_result['errors']}"
            assert scenario_result["business_rules_satisfied"], "ä¸šåŠ¡è§„åˆ™æœªæ»¡è¶³"
            assert scenario_result["resource_utilization"] >= 0.7, "èµ„æºåˆ©ç”¨ç‡è¿‡ä½"
            assert scenario_result["schedule_feasibility"], "æ’ç¨‹æ–¹æ¡ˆä¸å¯è¡Œ"
            
            print("âœ… çœŸå®æ•°æ®åœºæ™¯æµ‹è¯•é€šè¿‡")
            print(f"ğŸ“Š å¤„ç†è®¡åˆ’æ•°: {scenario_result['processed_plans']}")
            print(f"ğŸ­ èµ„æºåˆ©ç”¨ç‡: {scenario_result['resource_utilization']:.1%}")
            print(f"â±ï¸ å®Œæˆæ—¶é—´: {scenario_result['completion_time']:.2f}ç§’")
            print(f"âœ“ ä¸šåŠ¡è§„åˆ™æ»¡è¶³: {scenario_result['business_rules_satisfied']}")
            
        except Exception as e:
            pytest.fail(f"çœŸå®æ•°æ®åœºæ™¯æµ‹è¯•å¤±è´¥: {str(e)}")

    def _create_realistic_scenario(self) -> Dict[str, Any]:
        """åˆ›å»ºçœŸå®ä¸šåŠ¡åœºæ™¯"""
        return {
            "scenario_name": "æµ™æ±Ÿä¸­çƒŸ12æœˆç”Ÿäº§è®¡åˆ’",
            "description": "æ¨¡æ‹ŸçœŸå®çš„æœˆåº¦ç”Ÿäº§è®¡åˆ’åœºæ™¯",
            "plans": self.test_plans,
            "machines": self.test_machines,
            "calendar": self.test_calendar,
            "business_constraints": {
                "brand_priority": {"åˆ©ç¾¤": 1, "é»„é‡‘å¶": 2, "çœŸé¾™": 3, "å¤§é‡ä¹": 4, "è¥¿æ¹–": 5},
                "factory_rules": {
                    "max_daily_batches": 8,
                    "min_batch_interval": 30,  # åˆ†é’Ÿ
                    "quality_check_time": 15   # åˆ†é’Ÿ
                },
                "resource_limits": {
                    "feeder_max_utilization": 0.90,
                    "maker_max_utilization": 0.85,
                    "shared_resources": ["quality_inspector", "material_handler"]
                }
            },
            "kpis": {
                "target_efficiency": 0.85,
                "target_utilization": 0.80,
                "max_overtime_ratio": 0.15,
                "on_time_delivery": 0.95
            }
        }

    async def _execute_realistic_scenario(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒçœŸå®åœºæ™¯æµ‹è¯•"""
        start_time = time.time()
        errors = []
        warnings = []
        
        try:
            # 1. éªŒè¯è¾“å…¥æ•°æ®å®Œæ•´æ€§
            plans = scenario["plans"]
            machines = scenario["machines"]
            calendar = scenario["calendar"]
            
            # 2. æ‰§è¡Œä¸šåŠ¡è§„åˆ™éªŒè¯
            business_validation = await self._validate_business_rules(scenario)
            
            # 3. æ‰§è¡Œèµ„æºåˆ†é…ä¼˜åŒ–
            resource_allocation = await self._optimize_resource_allocation(scenario)
            
            # 4. ç”Ÿæˆæ’ç¨‹æ–¹æ¡ˆ
            schedule_generation = await self._generate_production_schedule(scenario)
            
            # 5. éªŒè¯KPIè¾¾æˆæƒ…å†µ
            kpi_validation = await self._validate_kpi_achievement(scenario)
            
            completion_time = time.time() - start_time
            
            # è®¡ç®—ç»¼åˆç»“æœ
            overall_success = all([
                business_validation["success"],
                resource_allocation["success"],
                schedule_generation["success"],
                kpi_validation["success"]
            ])
            
            return {
                "success": overall_success,
                "processed_plans": len(plans),
                "resource_utilization": resource_allocation.get("utilization", 0.0),
                "schedule_feasibility": schedule_generation.get("feasible", False),
                "business_rules_satisfied": business_validation.get("satisfied", False),
                "kpi_achievement": kpi_validation.get("achievement_rate", 0.0),
                "completion_time": completion_time,
                "errors": errors,
                "warnings": warnings,
                "detailed_results": {
                    "business_validation": business_validation,
                    "resource_allocation": resource_allocation,
                    "schedule_generation": schedule_generation,
                    "kpi_validation": kpi_validation
                }
            }
            
        except Exception as e:
            errors.append(str(e))
            return {
                "success": False,
                "processed_plans": 0,
                "resource_utilization": 0.0,
                "schedule_feasibility": False,
                "business_rules_satisfied": False,
                "kpi_achievement": 0.0,
                "completion_time": time.time() - start_time,
                "errors": errors,
                "warnings": warnings
            }

    async def _validate_business_rules(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯ä¸šåŠ¡è§„åˆ™"""
        # æ¨¡æ‹Ÿä¸šåŠ¡è§„åˆ™éªŒè¯
        await asyncio.sleep(0.1)
        
        constraints = scenario["business_constraints"]
        brand_priorities = constraints["brand_priority"]
        
        # éªŒè¯å“ç‰Œä¼˜å…ˆçº§æ’åº
        plans = scenario["plans"]
        priority_violations = 0
        
        for plan in plans:
            brand = plan.article_name.split("ï¼ˆ")[0] if "ï¼ˆ" in plan.article_name else plan.article_name
            if brand in brand_priorities:
                expected_priority = brand_priorities[brand]
                # ç®€åŒ–éªŒè¯é€»è¾‘
                if plan.priority.value != expected_priority:
                    priority_violations += 1
        
        return {
            "success": priority_violations < len(plans) * 0.2,  # å…è®¸20%çš„è¿è§„
            "satisfied": priority_violations == 0,
            "priority_violations": priority_violations,
            "validation_details": {
                "brand_priority_check": "completed",
                "factory_rules_check": "completed",
                "constraint_validation": "completed"
            }
        }

    async def _optimize_resource_allocation(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼˜åŒ–èµ„æºåˆ†é…"""
        # æ¨¡æ‹Ÿèµ„æºåˆ†é…ä¼˜åŒ–
        await asyncio.sleep(0.2)
        
        machines = scenario["machines"]
        plans = scenario["plans"]
        
        # è®¡ç®—æ€»å®¹é‡å’Œéœ€æ±‚
        total_feeder_capacity = sum(
            m.capacity_per_hour * m.efficiency_factor 
            for m in machines if m.machine_type == MachineType.FEEDER
        )
        total_maker_capacity = sum(
            m.capacity_per_hour * m.efficiency_factor 
            for m in machines if m.machine_type == MachineType.MAKER
        )
        
        total_demand = sum(plan.target_quantity for plan in plans)
        
        # è®¡ç®—åˆ©ç”¨ç‡
        working_hours = scenario["calendar"]["total_working_hours"]
        feeder_utilization = total_demand / (total_feeder_capacity * working_hours)
        maker_utilization = total_demand / (total_maker_capacity * working_hours)
        
        avg_utilization = (feeder_utilization + maker_utilization) / 2
        
        return {
            "success": avg_utilization <= 1.0,  # ä¸è¶…è¿‡100%åˆ©ç”¨ç‡
            "utilization": min(avg_utilization, 1.0),
            "feeder_utilization": feeder_utilization,
            "maker_utilization": maker_utilization,
            "allocation_details": {
                "total_demand": total_demand,
                "total_capacity": (total_feeder_capacity + total_maker_capacity) / 2,
                "working_hours": working_hours
            }
        }

    async def _generate_production_schedule(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç”Ÿäº§æ’ç¨‹"""
        # æ¨¡æ‹Ÿæ’ç¨‹ç”Ÿæˆ
        await asyncio.sleep(0.3)
        
        plans = scenario["plans"]
        machines = scenario["machines"]
        
        # ç®€åŒ–çš„æ’ç¨‹åˆ†é…
        schedules = []
        for i, plan in enumerate(plans):
            feeder = machines[i % 3] if machines[i % 3].machine_type == MachineType.FEEDER else machines[0]
            maker = machines[(i % 3) + 3] if len(machines) > 3 and machines[(i % 3) + 3].machine_type == MachineType.MAKER else machines[-1]
            
            schedule = ScheduleResult(
                schedule_id=generate_schedule_id(plan.plan_id),
                plan_id=plan.plan_id,
                assigned_feeder=feeder.machine_code,
                assigned_maker=maker.machine_code,
                scheduled_start=plan.planned_start or datetime.now(),
                scheduled_end=plan.planned_end or datetime.now() + timedelta(hours=8),
                allocated_quantity=plan.target_quantity,
                estimated_duration=8.0,
                priority_score=calculate_priority_score(plan),
                constraints_satisfied=True
            )
            schedules.append(schedule)
        
        return {
            "success": len(schedules) == len(plans),
            "feasible": True,
            "generated_schedules": len(schedules),
            "schedule_quality": 0.85,  # æ¨¡æ‹Ÿè´¨é‡åˆ†æ•°
            "schedules": [s.to_dict() for s in schedules]
        }

    async def _validate_kpi_achievement(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯KPIè¾¾æˆæƒ…å†µ"""
        # æ¨¡æ‹ŸKPIéªŒè¯
        await asyncio.sleep(0.1)
        
        kpis = scenario["kpis"]
        
        # æ¨¡æ‹ŸKPIè®¡ç®—ç»“æœ
        achieved_kpis = {
            "efficiency": 0.83,       # ç›®æ ‡: 0.85
            "utilization": 0.82,      # ç›®æ ‡: 0.80
            "overtime_ratio": 0.12,   # ç›®æ ‡: 0.15
            "on_time_delivery": 0.96  # ç›®æ ‡: 0.95
        }
        
        # è®¡ç®—è¾¾æˆç‡
        achievement_scores = []
        for kpi_name, target in kpis.items():
            if kpi_name.startswith("target_"):
                kpi_key = kpi_name.replace("target_", "")
            elif kpi_name.startswith("max_"):
                kpi_key = kpi_name.replace("max_", "") + "_ratio"
            else:
                kpi_key = kpi_name
                
            if kpi_key in achieved_kpis:
                achieved = achieved_kpis[kpi_key]
                if "max_" in kpi_name:
                    # è¶Šå°è¶Šå¥½çš„æŒ‡æ ‡
                    score = 1.0 if achieved <= target else target / achieved
                else:
                    # è¶Šå¤§è¶Šå¥½çš„æŒ‡æ ‡
                    score = achieved / target if target > 0 else 1.0
                achievement_scores.append(min(1.0, score))
        
        overall_achievement = sum(achievement_scores) / len(achievement_scores) if achievement_scores else 0.0
        
        return {
            "success": overall_achievement >= 0.8,
            "achievement_rate": overall_achievement,
            "individual_kpis": {
                "efficiency": achieved_kpis["efficiency"] / kpis["target_efficiency"],
                "utilization": achieved_kpis["utilization"] / kpis["target_utilization"],
                "overtime": kpis["max_overtime_ratio"] / achieved_kpis["overtime_ratio"],
                "delivery": achieved_kpis["on_time_delivery"] / kpis["on_time_delivery"]
            },
            "achieved_values": achieved_kpis,
            "target_values": kpis
        }

    def _verify_pipeline_data_consistency(self, pipeline_result: PipelineExecutionResult):
        """éªŒè¯ç®¡é“æ•°æ®ä¸€è‡´æ€§"""
        # éªŒè¯å¤„ç†çš„è®¡åˆ’æ•°é‡ä¸€è‡´æ€§
        assert pipeline_result.processed_plans == len(self.test_plans), \
            f"å¤„ç†è®¡åˆ’æ•°ä¸ä¸€è‡´: {pipeline_result.processed_plans} vs {len(self.test_plans)}"
        
        # éªŒè¯ç®—æ³•ç»“æœçš„æ•°æ®å®Œæ•´æ€§
        algorithm_results = pipeline_result.algorithm_results
        
        for alg_name, result_data in algorithm_results.items():
            assert "result" in result_data, f"ç®—æ³• {alg_name} ç¼ºå°‘ç»“æœæ•°æ®"
            assert "execution_time" in result_data, f"ç®—æ³• {alg_name} ç¼ºå°‘æ‰§è¡Œæ—¶é—´"
            assert result_data["execution_time"] >= 0, f"ç®—æ³• {alg_name} æ‰§è¡Œæ—¶é—´æ— æ•ˆ"
        
        # éªŒè¯é”™è¯¯å’Œè­¦å‘Šä¿¡æ¯çš„åˆç†æ€§
        assert len(pipeline_result.errors) < 5, f"é”™è¯¯è¿‡å¤š: {len(pipeline_result.errors)}"
        assert len(pipeline_result.warnings) < 10, f"è­¦å‘Šè¿‡å¤š: {len(pipeline_result.warnings)}"

    def _generate_performance_report(self, benchmark_results: List[Dict[str, Any]]):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "test_summary": {
                "total_benchmarks": len(benchmark_results),
                "successful_benchmarks": len([r for r in benchmark_results if r.get("meets_time_target") and r.get("meets_memory_target")]),
                "performance_trends": []
            },
            "benchmark_details": benchmark_results,
            "recommendations": []
        }
        
        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        successful_results = [r for r in benchmark_results if "execution_time" in r]
        if successful_results:
            avg_throughput = sum(r.get("throughput", 0) for r in successful_results) / len(successful_results)
            max_memory = max(r.get("memory_usage_mb", 0) for r in successful_results)
            
            report_data["test_summary"]["average_throughput"] = avg_throughput
            report_data["test_summary"]["peak_memory_usage"] = max_memory
            
            # ç”Ÿæˆå»ºè®®
            if avg_throughput < 5.0:
                report_data["recommendations"].append("è€ƒè™‘ä¼˜åŒ–ç®—æ³•å¹¶è¡Œåº¦ä»¥æé«˜ååé‡")
            if max_memory > 150:
                report_data["recommendations"].append("ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œè€ƒè™‘å¢åŠ å†…å­˜ç®¡ç†æœºåˆ¶")
        
        # å°†æŠ¥å‘Šæ·»åŠ åˆ°æµ‹è¯•è®°å½•
        self.test_reporter.add_test_result(
            "performance_benchmark",
            True,
            report_data,
            None
        )

    def teardown_method(self):
        """æµ‹è¯•æ¸…ç†"""
        # ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æŠ¥å‘Š
        final_report = self.test_reporter.generate_report()
        
        # æ¸…ç†èµ„æº
        gc.collect()
        
        print("\nğŸ“‹ T015æµ‹è¯•ä¼šè¯æ€»ç»“:")
        print(f"  æ€»æµ‹è¯•æ•°: {final_report['summary']['total_tests']}")
        print(f"  æˆåŠŸç‡: {final_report['summary']['success_rate']:.1%}")
        print(f"  æ‰§è¡Œæ—¶é—´: {final_report['test_session']['total_duration_seconds']:.2f}ç§’")
        
        if final_report.get("performance_summary"):
            perf = final_report["performance_summary"]
            print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {perf.get('avg_execution_time', 0):.2f}ç§’")
            print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {perf.get('avg_memory_usage', 0):.1f}MB")


# =============================================================================
# ç‹¬ç«‹è¿è¡Œå’Œå·¥å…·å‡½æ•°
# =============================================================================

def test_pipeline_configuration_validity():
    """æµ‹è¯•ç®¡é“é…ç½®æœ‰æ•ˆæ€§"""
    # éªŒè¯ç®—æ³•ç®¡é“é…ç½®
    assert len(ALGORITHM_PIPELINE) > 0, "ç®—æ³•ç®¡é“ä¸ºç©º"
    assert len(ALGORITHM_MODULES) > 0, "ç®—æ³•æ¨¡å—é…ç½®ä¸ºç©º"
    
    # éªŒè¯ç®¡é“é¡ºåºä¸­çš„æ¯ä¸ªç®—æ³•éƒ½æœ‰é…ç½®
    for alg_name in ALGORITHM_PIPELINE:
        assert alg_name in ALGORITHM_MODULES, f"ç®—æ³• {alg_name} ç¼ºå°‘é…ç½®ä¿¡æ¯"
        
        module_info = ALGORITHM_MODULES[alg_name]
        assert "name" in module_info, f"ç®—æ³• {alg_name} ç¼ºå°‘åç§°"
        assert "description" in module_info, f"ç®—æ³• {alg_name} ç¼ºå°‘æè¿°"
        assert "dependencies" in module_info, f"ç®—æ³• {alg_name} ç¼ºå°‘ä¾èµ–ä¿¡æ¯"


def test_base_algorithm_functionality():
    """æµ‹è¯•åŸºç¡€ç®—æ³•ç±»åŠŸèƒ½"""
    # åˆ›å»ºæµ‹è¯•ç®—æ³•å®ä¾‹
    class TestAlgorithm(BaseAlgorithm):
        async def execute(self, input_data, **kwargs):
            return {"result": "test_output", "input": input_data}
        
        def validate_input(self, input_data):
            return input_data is not None
    
    # æµ‹è¯•ç®—æ³•å®ä¾‹åŒ–
    test_alg = TestAlgorithm(AlgorithmType.CALENDAR_SERVICE)
    assert test_alg.algorithm_type == AlgorithmType.CALENDAR_SERVICE
    assert test_alg.version == "1.0.0"
    
    # æµ‹è¯•è¾“å…¥éªŒè¯
    assert test_alg.validate_input("test_data") == True
    assert test_alg.validate_input(None) == False
    
    # æµ‹è¯•ç®—æ³•ä¿¡æ¯
    info = test_alg.get_info()
    assert "algorithm_type" in info
    assert "version" in info
    assert "config" in info


if __name__ == "__main__":
    # ç‹¬ç«‹è¿è¡ŒT015æµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short"])
    
    print("\n" + "="*80)
    print("ğŸ¯ T015 - æœˆåº¦ç®—æ³•ç®¡é“æ‰§è¡Œé›†æˆæµ‹è¯•å®Œæˆ")
    print("âœ… æµ‹è¯•å†…å®¹:")
    print("   â€¢ å®Œæ•´ç®—æ³•ç®¡é“æ‰§è¡Œæµç¨‹")
    print("   â€¢ ç®—æ³•é—´ä¾èµ–å…³ç³»å’Œæ•°æ®æµè½¬")
    print("   â€¢ å¹¶è¡Œæ‰§è¡Œå’Œæ€§èƒ½ä¼˜åŒ–")
    print("   â€¢ çº¦æŸéµå¾ªå’Œç»“æœéªŒè¯")
    print("   â€¢ å¼‚å¸¸å¤„ç†å’Œé”™è¯¯æ¢å¤")
    print("   â€¢ å†…å­˜å’Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("   â€¢ å·¥ä½œæ—¥å†é›†æˆæµ‹è¯•")
    print("   â€¢ çœŸå®æ•°æ®åœºæ™¯æµ‹è¯•")
    print("ğŸ“Š è¦†ç›–èŒƒå›´: 8ä¸ªç®—æ³•æ¨¡å— + å®Œæ•´ç®¡é“åè°ƒ")
    print("ğŸš€ æ€§èƒ½éªŒè¯: ååé‡ã€å†…å­˜ä½¿ç”¨ã€æ‰§è¡Œæ—¶é—´åŸºå‡†")
    print("="*80)